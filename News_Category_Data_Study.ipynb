{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Category Data Review\n",
    "\n",
    "Load the news category data into the memory. It takes a few minutes and please be pacient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wangdi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "News category data load completed. Total 200847 recorders.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import Run_Prediction as rp\n",
    "import NewsCategoryData as ncd\n",
    "from NewsCategoryData import NewsCategory\n",
    "from NewsCategoryData import LABEL_DIC\n",
    "from NewsCategoryData import LABEL_LIST\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "data = NewsCategory(batch_size=BATCH_SIZE)\n",
    "print(\"News category data load completed. Total %d recorders.\"%len(data.data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run below codes to print all the labels and label index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CRIME</td>\n",
       "      <td>0</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>1</td>\n",
       "      <td>WORLD NEWS</td>\n",
       "      <td>2</td>\n",
       "      <td>IMPACT</td>\n",
       "      <td>3</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BLACK VOICES</td>\n",
       "      <td>6</td>\n",
       "      <td>WOMEN</td>\n",
       "      <td>7</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>8</td>\n",
       "      <td>QUEER VOICES</td>\n",
       "      <td>9</td>\n",
       "      <td>SPORTS</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>12</td>\n",
       "      <td>MEDIA</td>\n",
       "      <td>13</td>\n",
       "      <td>TECH</td>\n",
       "      <td>14</td>\n",
       "      <td>RELIGION</td>\n",
       "      <td>15</td>\n",
       "      <td>SCIENCE</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EDUCATION</td>\n",
       "      <td>18</td>\n",
       "      <td>COLLEGE</td>\n",
       "      <td>19</td>\n",
       "      <td>PARENTS</td>\n",
       "      <td>20</td>\n",
       "      <td>ARTS &amp; CULTURE</td>\n",
       "      <td>21</td>\n",
       "      <td>STYLE</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TASTE</td>\n",
       "      <td>24</td>\n",
       "      <td>HEALTHY LIVING</td>\n",
       "      <td>25</td>\n",
       "      <td>THE WORLDPOST</td>\n",
       "      <td>26</td>\n",
       "      <td>GOOD NEWS</td>\n",
       "      <td>27</td>\n",
       "      <td>WORLDPOST</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ARTS</td>\n",
       "      <td>30</td>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>31</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>32</td>\n",
       "      <td>HOME &amp; LIVING</td>\n",
       "      <td>33</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0   1               2   3              4   5               6  \\\n",
       "0         CRIME   0   ENTERTAINMENT   1     WORLD NEWS   2          IMPACT   \n",
       "1  BLACK VOICES   6           WOMEN   7         COMEDY   8    QUEER VOICES   \n",
       "2        TRAVEL  12           MEDIA  13           TECH  14        RELIGION   \n",
       "3     EDUCATION  18         COLLEGE  19        PARENTS  20  ARTS & CULTURE   \n",
       "4         TASTE  24  HEALTHY LIVING  25  THE WORLDPOST  26       GOOD NEWS   \n",
       "5          ARTS  30        WELLNESS  31      PARENTING  32   HOME & LIVING   \n",
       "\n",
       "    7               8   9  \n",
       "0   3        POLITICS   4  \n",
       "1   9          SPORTS  10  \n",
       "2  15         SCIENCE  16  \n",
       "3  21           STYLE  22  \n",
       "4  27       WORLDPOST  28  \n",
       "5  33  STYLE & BEAUTY  34  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "od = ncd.print_all_class()\n",
    "od"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run below code to show 10 news title at specific label\n",
    "Input the index of label you want to review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Please input a label index(0-40): 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPACT   With Its Way Of Life At Risk , This Remote Oyster - Growing Region Called In Robots \n",
      "IMPACT   Monsanto And Bayer Are Set To Merge . Heres Why You Should Care . \n",
      "IMPACT   Youre Going To Use That Self - Checkout Machine Whether You Like It Or Not \n",
      "IMPACT   Machines Dont Always Steal Our Jobs . Increasingly , They Rob Our Work Of Humanity . \n",
      "IMPACT   She Started A Suicide Prevention Site At Age 15 . Its Still Going Strong . \n",
      "IMPACT   This CEO Took A Pay Cut To Give Employees $ 70 , 000 A Year . Now He ’ s Battling Amazon . \n",
      "IMPACT   Social Anxiety Doesn ’ t Get The Attention It Deserves \n",
      "IMPACT   The Battle To Save Our Dying Soil \n",
      "IMPACT   A New Farming Technique Using Drastically Less Water Is Catching On \n",
      "IMPACT   6 Incredible Photos That Show The World We Need To Protect \n",
      "IMPACT   Our Snack Addiction Is Killing Orangutans \n"
     ]
    }
   ],
   "source": [
    "MAX_SHOW = 10\n",
    "\n",
    "def is_int(s):\n",
    "    try:\n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    "\n",
    "index = 0\n",
    "\n",
    "while True:\n",
    "    input_value = input(\"Please input a label index(0-40):\") \n",
    "    if is_int(input_value):\n",
    "        index = int(input_value)\n",
    "        if index <=40 or index >=0:\n",
    "            break\n",
    "        \n",
    "icount = 0\n",
    "\n",
    "for i in range(data.max_recorder):\n",
    "    if data.label[i] == index:\n",
    "        news_title = \"\"\n",
    "        for word in data.data[i]:\n",
    "            news_title += word + \" \"\n",
    "        print(LABEL_LIST[data.label[i]],\" \", news_title)\n",
    "        icount += 1\n",
    "        if icount > MAX_SHOW:\n",
    "            break\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate a model accuracy\n",
    "The result will be saved in list **prediction** and text file **prediction.log**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-a9be52cf94b7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mMODEL_FILE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'news_category-100000.meta'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mckpt_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./ckpt_save/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mprediction_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cnn_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckpt_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mMODEL_FILE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m#print(len(prediction_1))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Notebook\\NewsCategory\\Run_Prediction.py\u001b[0m in \u001b[0;36mrun_cnn_model\u001b[1;34m(ckpt_dir, model_file)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[1;31m#ckpt_dir = './ckpt_save/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNewsCategory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[0mprediction_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[0mmax_recorder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Notebook\\NewsCategory\\NewsCategoryData.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath, batch_size, max_length, shuffle)\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_recorder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_recorder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Notebook\\NewsCategory\\NewsCategoryData.py\u001b[0m in \u001b[0;36m_read_file\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    171\u001b[0m                         \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mtokenized_word\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                     \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLABEL_DIC\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mjsonText\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'category'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                 \u001b[0mjsonData\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[1;31m#print(len(data))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[1;31m#print(len(label))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "def save_prediction(prediction, file_name):\n",
    "    fo = open(file_name, \"w+\")\n",
    "    for item in prediction:\n",
    "        fo. write(str(item)+\"\\n\")\n",
    "    fo.close()\n",
    "MODEL_FILE = 'news_category-100000.meta'\n",
    "ckpt_dir = './ckpt_save/'\n",
    "prediction_1 = rp.run_cnn_model(ckpt_dir,MODEL_FILE)\n",
    "#print(len(prediction_1))\n",
    "\n",
    "word_matrix_file = \"naive_bayes_word_matrix.csv\"\n",
    "prediction_2 = rp.run_naive_bayes_model(word_matrix_file)\n",
    "#print(len(prediction_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from NewsCategoryData import NewsCategoryTrainTestSet \n",
    "word_matrix_file = \"naive_bayes_word_matrix_ver1.csv\"\n",
    "word_matrix, word_list = rp.read_naive_bayes_word_vector(word_matrix_file)\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = datapath('c:/Notebook/GloVe/glove.6B.100d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
    "\n",
    "def run_combined_model(data, ckpt_dir, graph_file, word_matrix, word_list):\n",
    "    graph = tf.Graph()\n",
    "    with graph.as_default():\n",
    "        saver = tf.train.import_meta_graph(ckpt_dir + graph_file)\n",
    "        #import tensor variables\n",
    "        x_rnn = graph.get_tensor_by_name(\"input/x_rnn:0\")\n",
    "        x_naive_bayes = graph.get_tensor_by_name(\"input/x_naive_bayes:0\")\n",
    "        x_length = graph.get_tensor_by_name(\"input/x_length:0\")\n",
    "        prediction = graph.get_tensor_by_name(\"Prediction/prediction:0\")\n",
    "    \n",
    "        with tf.Session() as sess:\n",
    "            model_file=tf.train.latest_checkpoint(ckpt_dir)\n",
    "            saver.restore(sess,model_file)\n",
    "            batch_data_vec, data_length = ncd.word2vector(data, model, 100)\n",
    "            naive_bayes_input = rp.naive_bayes_model(batch_data, word_matrix, word_list)\n",
    "            y= sess.run([prediction],\n",
    "                        feed_dict={x_rnn:batch_data_vec, \n",
    "                                   x_naive_bayes:naive_bayes_input,\n",
    "                                   x_length:data_length})\n",
    "\n",
    "    return y[0]\n",
    "\n",
    "data_path = \"news_cat_train_test_data.csv\"\n",
    "ckpt_dir = \"./ckpt_save_combine_model/\"\n",
    "graph_file = \"news_category-100000.meta\"\n",
    "\n",
    "train_test_data = NewsCategoryTrainTestSet(filepath=data_path, batch_size=100)\n",
    "prediction_list = []\n",
    "for i in range(int(train_test_data.test_size/100)+1):\n",
    "    batch_data, _ = train_test_data.batch_test_set()\n",
    "    prediction = run_combined_model(batch_data, ckpt_dir, graph_file, word_matrix, word_list)\n",
    "    #print(prediction)\n",
    "    prediction_list = prediction_list + [i  for i in prediction]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52.42697177453485\n",
      "62.20356788998591\n"
     ]
    }
   ],
   "source": [
    "def cal_accuracy(data, prediction):\n",
    "    accuracy = 0\n",
    "    for i  in range(data.max_recorder):\n",
    "        if prediction[i] == data.label[i]:\n",
    "            accuracy += 1\n",
    "    return accuracy/data.max_recorder\n",
    "print(cal_accuracy(data,prediction_1)*100)\n",
    "print(cal_accuracy(data,prediction_2)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the accuracy of each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prediction_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-2e85eb241df8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;31m#check_accuracy_by_label(prediction_1,data.label)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;31m#check_accuracy_by_label(prediction_2,data.label)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mcheck_accuracy_by_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprediction_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_test_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'prediction_list' is not defined"
     ]
    }
   ],
   "source": [
    "def check_accuracy_by_label(prediction, label):\n",
    "    count_correct_by_category = [0 for item in LABEL_DIC]\n",
    "    count_total_by_category = [0 for item in LABEL_DIC]\n",
    "\n",
    "    for i in range(len(label)):\n",
    "        count_total_by_category[label[i]] += 1\n",
    "        if label[i] == prediction[i]:\n",
    "            count_correct_by_category[label[i]] += 1\n",
    "    accuracy = []\n",
    "    #print(count_correct_by_category)\n",
    "    #print(count_total_by_category)\n",
    "    for i in range(len(count_correct_by_category)):\n",
    "        accuracy.append(count_correct_by_category[i]/count_total_by_category[i])\n",
    "\n",
    "    TRAIN_SAMPLE_LEVEL = [5000,2000,1000,500,200]\n",
    "    train_sample_level = [(float('inf'), TRAIN_SAMPLE_LEVEL[0])]\n",
    "    for i in range(len(TRAIN_SAMPLE_LEVEL)-1):\n",
    "        train_sample_level.append((TRAIN_SAMPLE_LEVEL[i],TRAIN_SAMPLE_LEVEL[i+1]))\n",
    "    train_sample_level.append((TRAIN_SAMPLE_LEVEL[len(TRAIN_SAMPLE_LEVEL)-1],0))\n",
    "    print(train_sample_level)\n",
    "\n",
    "    for level in train_sample_level:\n",
    "        for i in range(len(count_correct_by_category)):\n",
    "            if count_total_by_category[i] <= level[0] and count_total_by_category[i] > level[1]:\n",
    "                print(\"%s:\\t  %.2f %% \\t %d \\t%.2f %%\"%(LABEL_LIST[i],accuracy[i]*100, count_total_by_category[i],count_total_by_category[i]/len(label)*100))\n",
    "        print(\"*\"*20)\n",
    "#check_accuracy_by_label(prediction_1,data.label)\n",
    "#check_accuracy_by_label(prediction_2,data.label)\n",
    "check_accuracy_by_label(prediction_list, train_test_data.test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the naive bayes word vectors. It takes a few minutes.\n",
      "[[2.6427e-05 3.1536e-04 0.0000e+00 1.4195e-04 3.4879e-03 1.7384e-04\n",
      "  2.6860e-04 1.5240e-03 8.8841e-04 2.8942e-04 7.1722e-05 1.4702e-04\n",
      "  0.0000e+00 9.5298e-04 4.5360e-05 1.2278e-04 0.0000e+00 5.3821e-04\n",
      "  2.0672e-04 1.7493e-04 2.2368e-04 5.7618e-04 3.7574e-04 1.1148e-04\n",
      "  1.4270e-04 6.2869e-05 4.9418e-05 1.2218e-04 4.0393e-05 3.0862e-04\n",
      "  0.0000e+00 0.0000e+00 1.1541e-05 0.0000e+00 2.3623e-04 0.0000e+00\n",
      "  5.2102e-05 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [5.2854e-04 6.8508e-04 2.8423e-04 5.6779e-05 2.6386e-04 2.4338e-04\n",
      "  3.2615e-04 8.3126e-05 1.0452e-04 2.3430e-04 1.9724e-04 2.1237e-04\n",
      "  2.1262e-04 1.9717e-04 3.1752e-04 1.2278e-04 1.6947e-04 2.3066e-04\n",
      "  2.0672e-04 0.0000e+00 1.2427e-04 2.5608e-04 4.1748e-05 1.4864e-04\n",
      "  4.7567e-05 1.1002e-04 2.4709e-04 6.1091e-05 4.4432e-04 7.7155e-05\n",
      "  2.0138e-04 1.4066e-04 1.3849e-04 3.9460e-04 1.7106e-04 1.1379e-04\n",
      "  1.8236e-04 1.2391e-04 1.1193e-04 1.3905e-04 2.6187e-04]\n",
      " [1.7125e-02 1.7399e-02 2.2576e-02 2.8333e-02 2.6957e-02 1.6376e-02\n",
      "  2.2620e-02 2.6850e-02 1.6340e-02 2.3347e-02 1.9616e-02 2.5615e-02\n",
      "  2.5663e-02 2.2017e-02 2.5447e-02 2.3124e-02 1.8854e-02 2.7603e-02\n",
      "  2.5013e-02 3.1488e-02 3.0271e-02 2.2215e-02 2.0373e-02 2.2185e-02\n",
      "  2.9016e-02 3.4138e-02 2.1843e-02 2.9140e-02 1.9469e-02 3.2636e-02\n",
      "  1.3828e-02 2.7454e-02 2.7133e-02 2.2433e-02 1.4166e-02 2.6144e-02\n",
      "  2.6468e-02 2.2799e-02 3.0108e-02 1.8285e-02 9.0782e-03]\n",
      " [1.0571e-04 8.6995e-05 3.6544e-04 3.4068e-04 2.7210e-04 6.9536e-05\n",
      "  1.3430e-04 8.3126e-05 3.4840e-05 2.7564e-04 2.5103e-04 2.4504e-04\n",
      "  7.4417e-05 1.6431e-04 2.2680e-04 2.0463e-04 2.1184e-04 0.0000e+00\n",
      "  4.1344e-04 5.2480e-04 9.9413e-05 6.4020e-05 3.3399e-04 1.4864e-04\n",
      "  1.4270e-04 3.7722e-04 2.4709e-04 2.4436e-04 2.0196e-04 4.6293e-04\n",
      "  1.3426e-04 2.8744e-04 1.3849e-04 2.5649e-04 8.1459e-05 4.5518e-04\n",
      "  2.6051e-05 1.0621e-04 1.1193e-04 1.3905e-04 8.7291e-05]\n",
      " [5.2854e-05 1.9574e-04 3.6544e-04 2.2712e-04 2.0614e-04 0.0000e+00\n",
      "  3.4534e-04 2.2167e-04 1.0452e-04 1.5160e-04 1.9724e-04 2.2870e-04\n",
      "  2.2325e-04 9.8584e-05 1.3608e-04 1.6371e-04 4.2367e-04 7.6888e-05\n",
      "  1.0336e-04 3.4986e-04 1.4912e-04 1.9206e-04 1.2525e-04 2.2297e-04\n",
      "  2.8540e-04 2.0433e-04 2.2238e-04 6.1091e-05 2.8275e-04 3.8577e-04\n",
      "  2.6851e-04 2.3852e-04 1.0387e-04 1.3811e-04 1.1404e-04 1.7069e-04\n",
      "  5.2102e-05 3.5403e-05 5.5963e-05 0.0000e+00 8.7291e-05]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 2.7485e-06 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [7.5846e-03 8.4331e-03 5.9688e-03 5.5644e-03 5.1232e-03 8.2400e-03\n",
      "  5.6789e-03 4.9044e-03 7.8041e-03 5.2234e-03 5.6660e-03 5.3418e-03\n",
      "  4.5713e-03 6.8023e-03 5.7607e-03 5.8116e-03 3.8131e-03 5.2284e-03\n",
      "  4.3411e-03 4.1984e-03 9.1212e-03 5.4417e-03 5.9283e-03 4.3850e-03\n",
      "  7.7534e-03 6.7742e-03 4.8183e-03 1.3745e-02 4.2412e-03 6.5581e-03\n",
      "  7.9211e-03 6.9109e-03 7.6634e-03 7.3001e-03 4.7165e-03 6.8561e-03\n",
      "  4.1421e-03 7.7355e-03 3.7495e-03 3.3373e-03 5.5866e-03]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 2.8390e-05 1.0994e-05 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 1.7930e-05 0.0000e+00\n",
      "  1.0631e-05 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [2.3784e-04 4.3497e-05 2.0302e-04 0.0000e+00 5.4970e-05 1.3907e-04\n",
      "  7.6742e-05 2.7709e-05 5.2259e-05 1.6538e-04 5.3791e-05 6.5343e-05\n",
      "  1.2119e-03 0.0000e+00 4.5360e-05 4.0927e-05 1.2710e-04 1.5378e-04\n",
      "  1.0336e-04 8.7466e-05 0.0000e+00 0.0000e+00 0.0000e+00 4.0877e-04\n",
      "  9.5134e-05 1.5717e-05 2.9651e-04 1.2218e-04 1.2118e-04 7.7155e-05\n",
      "  3.3564e-04 3.6695e-05 4.6165e-05 3.3541e-04 1.6292e-05 0.0000e+00\n",
      "  1.3026e-04 8.8507e-05 2.7981e-04 6.9527e-04 0.0000e+00]\n",
      " [2.6427e-05 2.1749e-05 2.0302e-04 2.2712e-04 2.1988e-04 0.0000e+00\n",
      "  7.6742e-05 1.1083e-04 1.5678e-04 5.5128e-05 5.3791e-05 9.8015e-05\n",
      "  0.0000e+00 1.6431e-04 4.5360e-05 1.2278e-04 0.0000e+00 7.6888e-05\n",
      "  1.0336e-04 4.3733e-04 0.0000e+00 6.4020e-05 4.1748e-05 1.4864e-04\n",
      "  0.0000e+00 7.8587e-05 1.2355e-04 0.0000e+00 8.0785e-05 0.0000e+00\n",
      "  0.0000e+00 4.8927e-05 0.0000e+00 1.9730e-05 3.2584e-05 2.8449e-05\n",
      "  0.0000e+00 0.0000e+00 5.5963e-05 6.9527e-05 0.0000e+00]\n",
      " [2.2992e-03 1.7671e-03 3.2483e-04 2.6119e-03 6.5965e-04 2.1904e-03\n",
      "  4.0481e-03 1.9396e-03 1.3344e-02 3.9555e-03 5.8633e-03 2.5811e-03\n",
      "  2.2899e-02 1.0516e-03 2.9030e-03 1.9235e-03 7.5838e-03 6.1510e-04\n",
      "  1.6537e-03 8.7466e-04 1.6900e-03 6.4020e-04 1.3359e-03 1.2635e-03\n",
      "  2.2832e-03 2.1847e-03 7.4127e-05 3.6655e-04 9.2903e-04 1.3888e-03\n",
      "  5.5716e-03 6.2748e-03 1.1195e-02 3.7270e-02 4.3972e-02 9.6441e-03\n",
      "  1.9408e-02 2.2870e-02 5.6522e-03 1.3975e-02 3.0988e-02]\n",
      " [9.2495e-04 2.7403e-03 2.6393e-03 4.0313e-03 3.4137e-03 3.9288e-03\n",
      "  2.3982e-03 5.3755e-03 2.9962e-03 3.0045e-03 2.1337e-03 4.3943e-03\n",
      "  2.7853e-03 2.4646e-03 3.9917e-03 4.1336e-03 3.1776e-03 4.5364e-03\n",
      "  4.6512e-03 5.7728e-03 5.0701e-03 4.8015e-03 4.3001e-03 4.0505e-03\n",
      "  6.4691e-03 5.1396e-03 2.5697e-03 1.9549e-03 2.6659e-03 3.0862e-03\n",
      "  3.0879e-03 3.9814e-03 4.5703e-03 2.7819e-03 2.6963e-03 2.9302e-03\n",
      "  2.8396e-03 3.7527e-03 4.3651e-03 2.5030e-03 1.3094e-03]\n",
      " [7.8224e-03 3.1345e-02 2.0099e-02 3.6254e-02 2.5611e-02 1.8914e-02\n",
      "  2.5670e-02 3.6437e-02 3.1251e-02 2.9466e-02 2.5963e-02 3.2378e-02\n",
      "  4.0982e-02 2.5205e-02 2.8259e-02 3.6056e-02 2.2836e-02 2.7680e-02\n",
      "  4.2584e-02 2.9651e-02 2.9973e-02 3.5980e-02 3.7949e-02 3.5712e-02\n",
      "  3.5533e-02 3.0287e-02 1.7148e-02 1.8327e-02 3.9989e-02 4.1509e-02\n",
      "  4.9473e-02 3.1270e-02 2.9949e-02 2.2788e-02 2.3599e-02 2.3925e-02\n",
      "  2.6702e-02 3.7899e-02 2.1937e-02 3.4068e-02 4.6875e-02]\n",
      " [0.0000e+00 2.7186e-05 0.0000e+00 0.0000e+00 5.2497e-04 0.0000e+00\n",
      "  3.8371e-05 1.9396e-04 1.2194e-04 1.3782e-05 0.0000e+00 1.6336e-05\n",
      "  0.0000e+00 9.8584e-05 0.0000e+00 4.0927e-05 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 4.9707e-05 6.4020e-05 8.3497e-05 3.7161e-05\n",
      "  9.5134e-05 1.5717e-05 2.4709e-05 0.0000e+00 0.0000e+00 1.5431e-04\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 5.7021e-05 0.0000e+00\n",
      "  2.6051e-05 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 5.4970e-06 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 1.5717e-05 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [5.5497e-04 4.4041e-04 7.3088e-04 7.6652e-04 7.4485e-04 4.5199e-04\n",
      "  3.2615e-04 8.8667e-04 8.7099e-04 7.0289e-04 5.1998e-04 1.2905e-03\n",
      "  7.9732e-04 4.6006e-04 4.9896e-04 1.2278e-03 5.9314e-04 6.9199e-04\n",
      "  1.2403e-03 1.3995e-03 8.6987e-04 6.4020e-04 5.4273e-04 8.5470e-04\n",
      "  8.5621e-04 1.4774e-03 2.9651e-04 3.0546e-04 7.2707e-04 7.7155e-04\n",
      "  8.0553e-04 1.7430e-03 1.5004e-03 1.1443e-03 1.1649e-03 1.8492e-03\n",
      "  8.3364e-04 1.1860e-03 1.8468e-03 4.1716e-04 2.6187e-04]\n",
      " [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 2.7485e-06 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      "  0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      " [1.4271e-03 2.2456e-03 3.2483e-03 6.2741e-03 3.7792e-03 5.1457e-03\n",
      "  2.4941e-03 5.4863e-03 3.6233e-03 5.2096e-03 2.9047e-03 7.1714e-03\n",
      "  5.4005e-03 3.7133e-03 5.1710e-03 7.7760e-03 7.9651e-03 2.7680e-03\n",
      "  1.3333e-02 6.2976e-03 4.6227e-03 2.7529e-03 3.1729e-03 5.3512e-03\n",
      "  3.6627e-03 7.7329e-03 1.5073e-03 1.6495e-03 7.9170e-03 5.1694e-03\n",
      "  4.3633e-03 1.3785e-02 1.0964e-02 3.5514e-03 5.6288e-03 1.5732e-02\n",
      "  7.8414e-03 5.6467e-03 1.0297e-02 5.7012e-03 1.6585e-03]\n",
      " [2.2992e-03 1.7671e-03 3.2483e-04 2.6119e-03 6.5415e-04 2.1904e-03\n",
      "  4.0481e-03 1.9396e-03 1.3361e-02 3.9555e-03 5.8453e-03 2.5811e-03\n",
      "  2.2899e-02 1.0516e-03 2.9030e-03 1.9235e-03 7.5838e-03 6.1510e-04\n",
      "  1.6537e-03 8.7466e-04 1.6900e-03 6.4020e-04 1.3359e-03 1.2635e-03\n",
      "  2.2832e-03 2.1847e-03 7.4127e-05 3.6655e-04 9.2903e-04 1.3888e-03\n",
      "  5.7730e-03 6.2748e-03 1.1195e-02 3.7250e-02 4.3963e-02 9.6726e-03\n",
      "  1.9408e-02 2.2852e-02 5.6522e-03 1.3975e-02 3.0988e-02]]\n",
      "x1 [0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 2.1932e-66 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00\n",
      " 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00]\n",
      "x2 [2.1932e-66]\n",
      "predict [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "4\n",
      "(41,)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import Run_Prediction as rp\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4,suppress =False)\n",
    "word_matrix_file = \"naive_bayes_word_matrix.csv\"\n",
    "word_matrix, word_list = rp.read_naive_bayes_word_vector(word_matrix_file)\n",
    "title_list = [['Hillary', 'Set', 'To', 'Move', 'Past', 'Prelims', 'With', 'Roosevelt', 'Island', 'Address', '(', 'Are', 'the', 'Clintons', 'Cynics', 'or', 'Realists', '?', ')']]\n",
    "for title in title_list:\n",
    "    naive_bayes_input = rp.naive_bayes_model(title, word_matrix, word_list)\n",
    "    print(naive_bayes_input)\n",
    "    print(naive_bayes_input.argmax())\n",
    "    print(naive_bayes_input.shape)\n",
    "    print(\"-\"*30)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Trainset and Testset \n",
    "\n",
    "For each category, 90% of recordes will be assigned as trainset and 10% will be assigned as test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3405, 16058, 2177, 3459, 32738, 2670, 4528, 3490, 5175, 6313, 4884, 5936, 9887, 2814, 2082, 2555, 2178, 1129, 1004, 1144, 3955, 1339, 2254, 2622, 2096, 6694, 3664, 1398, 2578, 1401, 1509, 17827, 8677, 4195, 9649, 3426, 3651, 6226, 1707, 1323, 1030]\n",
      "200847\n"
     ]
    }
   ],
   "source": [
    "# First step to get the count of recorders for each category.\n",
    "import csv\n",
    "def count_by_category(labels,class_mum):\n",
    "    count_by_cat_list = [0 for i in range(class_mum)]\n",
    "    for i in labels:\n",
    "        count_by_cat_list[i] += 1\n",
    "    return count_by_cat_list\n",
    "\n",
    "lst_count_by_category = count_by_category(data.label,len(LABEL_DIC))\n",
    "\n",
    "print(lst_count_by_category)\n",
    "\n",
    "total_data = 0\n",
    "for num in lst_count_by_category:\n",
    "    total_data += num\n",
    "print(total_data)\n",
    "\n",
    "#Generate trainset and testset\n",
    "train_data = []\n",
    "train_label = []\n",
    "test_data = []\n",
    "test_label = []\n",
    "count_train_by_category = [0 for i in range(len(LABEL_DIC))]\n",
    "\n",
    "for i in range(data.max_recorder):\n",
    "    if count_train_by_category[data.label[i]] <= lst_count_by_category[data.label[i]]*0.9:\n",
    "        train_data.append(data.data[i]) \n",
    "        train_label.append(data.label[i])\n",
    "        count_train_by_category[data.label[i]] += 1\n",
    "    else:\n",
    "        test_data.append(data.data[i]) \n",
    "        test_label.append(data.label[i])\n",
    "        \n",
    "# Now save the trainset and testset to the csv file.\n",
    "# For each recorder, there's label to mark whether it is belong to train or test set. \n",
    "fo = open(\"news_cat_train_test_data.csv\", \"w+\", encoding=\"utf-8\")\n",
    "writer = csv.writer(fo)\n",
    "writer.writerow(['title','label','train_or_test'])\n",
    "for i in range(len(train_data)):\n",
    "    writer.writerow([train_data[i],train_label[i],'train'])\n",
    "for i in range(len(test_data)):\n",
    "    writer.writerow([test_data[i],test_label[i],'test'])\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below is to test a class to read news category data which is seperated by train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200847\n",
      "['The', 'Buffett', 'Rule', 'or', 'the', 'Lincome', 'Tax', '?'] 38\n",
      "['Setting', 'Sail', 'On', 'A', 'Schooner', 'For', 'A', 'Knitting', 'Vacation'] 12\n"
     ]
    }
   ],
   "source": [
    "from NewsCategoryData import NewsCategoryTrainTestSet\n",
    "\n",
    "dataset = NewsCategoryTrainTestSet(filepath = \"news_cat_train_test_data.csv\")\n",
    "for i in range(10000):\n",
    "    train_data, train_label = dataset.batch_test_set()\n",
    "    if len(train_data) != 100 or len(train_label) != 100:\n",
    "        print(len(train_data),len(train_label))\n",
    "print(train_data[0], train_label[0])   \n",
    "\n",
    "for i in range(10000):\n",
    "    test_data, test_label = dataset.batch_test_set()\n",
    "    if len(test_data) != 100 or len(test_label) != 100:\n",
    "        print(len(test_data),len(test_label))\n",
    "print(test_data[0], test_label[0])      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Below code will run the combined model to give category prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-9374289aa1b2>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-9-9374289aa1b2>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    [\"police\", \"defend\",\"\" aboriginal tent embassy raid]]\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import Run_Prediction as rp\n",
    "\n",
    "ckpt_dir = './ckpt_save_8/'\n",
    "model_file='news_category-200000.meta'\n",
    "\n",
    "word_matrix_file = \"naive_bayes_word_matrix.csv\"\n",
    "word_matrix, word_list = rp.read_naive_bayes_word_vector(word_matrix_file)\n",
    "\n",
    "ckpt_dir_combine = './ckpt_save/'\n",
    "graph_file = 'combined_new_category-50000.meta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load graph.\n",
      "Load model.\n",
      "INFO:tensorflow:Restoring parameters from ./ckpt_save_8/news_category-200000\n",
      "./ckpt_save/combined_new_category-50000\n",
      "INFO:tensorflow:Restoring parameters from ./ckpt_save/combined_new_category-50000\n",
      "Restore model.\n",
      "2\n",
      "Load graph.\n",
      "Load model.\n",
      "INFO:tensorflow:Restoring parameters from ./ckpt_save_8/news_category-200000\n",
      "./ckpt_save/combined_new_category-50000\n",
      "INFO:tensorflow:Restoring parameters from ./ckpt_save/combined_new_category-50000\n",
      "Restore model.\n",
      "4\n",
      "Load graph.\n",
      "Load model.\n",
      "INFO:tensorflow:Restoring parameters from ./ckpt_save_8/news_category-200000\n",
      "./ckpt_save/combined_new_category-50000\n",
      "INFO:tensorflow:Restoring parameters from ./ckpt_save/combined_new_category-50000\n",
      "Restore model.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "title_list= [[\"aust\",\"addresses\",\"un\",\"security\",\"council\",\"over\", \"iraq\"],\n",
    "             [\"funds\",\"allocated\",\"for\",\"youth\",\"at\",\"risk\"],\n",
    "             [\"police\", \"defend\",\"aboriginal\",\"tent\",\"embassy\",\"raid\"]]\n",
    "for title in title_list:\n",
    "    cnn_logits = rp.one_cnn_model(title,ckpt_dir, model_file)\n",
    "    naive_bayes_input = rp.one_naive_bayes_model(title, word_matrix, word_list)\n",
    "    prediction = rp.one_combined_model(cnn_logits,naive_bayes_input,ckpt_dir_combine,graph_file)\n",
    "    print(prediction[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.18.1\n"
     ]
    }
   ],
   "source": [
    "# Find blank titles\n",
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
