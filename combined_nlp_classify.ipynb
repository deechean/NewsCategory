{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Combined end-to-end training model for NLP classify problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import threading\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "sys.path.append('../../common/')\n",
    "\n",
    "from train_log import train_log\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\" \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.FATAL)\n",
    "\n",
    "FILE_NAME = 'News_Category_Dataset_v2.json'\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = datapath('/home/ubuntu/Notebooks/GloVe/glove.6B.100d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
    "    \n",
    "import NewsCategoryData as ncd \n",
    "from NewsCategoryData import NewsCategoryTrainTestSet\n",
    "from NewsCategoryData import LABEL_LIST\n",
    "import Run_Prediction as rp\n",
    "\n",
    "CONTINUE = 0\n",
    "start_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 200847 recorders are read. 180787 train data and 20060 test data.\n",
      "Load the naive bayes word vectors. It takes a few minutes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "state_size = 64\n",
    "max_length = 100\n",
    "n_classes = 41\n",
    "word2vector_len = 100  \n",
    "data = NewsCategoryTrainTestSet(batch_size=batch_size,max_length=max_length)\n",
    "word_matrix_file = \"naive_bayes_word_matrix_ver1.csv\"\n",
    "word_matrix, word_list = rp.read_naive_bayes_word_vector(word_matrix_file)\n",
    "\n",
    "ckpt_dir = './ckpt_save_combine_model/'\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "log_dir = './log_combine_model/'\n",
    "log = train_log(log_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_window(word, title, window_size = 2):\n",
    "    '''\n",
    "    word: is the word you are looking for.\n",
    "    title: is a word list which contains the given word\n",
    "    window_size: decide how much word you are going to return, \n",
    "    for example 2 means only return the left and right words, \n",
    "    3 means return the left 2 and right 2 words. \n",
    "\n",
    "    '''\n",
    "    close_word_list = []\n",
    "    i = 0 \n",
    "    for i in range(len(title)):\n",
    "        if title[i].lower() == word:\n",
    "            if i > 0:\n",
    "                close_word_list.extend(title[max(i-window_size+1,0):i])\n",
    "            if i < len(title)-1:\n",
    "                close_word_list.extend(title[i+1:min(i+window_size,len(title))])\n",
    "    return close_word_list\n",
    "\n",
    "unknown_word = {}\n",
    "for title in data.train_data:\n",
    "    for word in title:\n",
    "        if model.vocab.get(word.lower(),\"NaN\") == \"NaN\":\n",
    "            if word.lower() in unknown_word:\n",
    "                unknown_word[word.lower()].append(title)\n",
    "            else:\n",
    "                unknown_word[word.lower()] = []\n",
    "                unknown_word[word.lower()].append(title)\n",
    "\n",
    "close_word_dict = {}\n",
    "for word in unknown_word:\n",
    "    close_word_dict[word] = []\n",
    "    for title in unknown_word[word]:\n",
    "        close_word_list = find_window(word, title, window_size = 3)\n",
    "        close_word_dict[word].extend(close_word_list)\n",
    "      \n",
    "    \n",
    "word_vector_dic = {}\n",
    "for word in close_word_dict:\n",
    "    word_vector_dic[word] =np.zeros(100)\n",
    "    for close_word in close_word_dict[word]:\n",
    "        if model.vocab.get(close_word.lower(),\"NaN\") != \"NaN\":\n",
    "            word_vector_dic[word] = np.add(word_vector_dic[word],model[close_word.lower()])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the out-of-vocabury words with the close words in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvector(word, model):\n",
    "    if model.vocab.get(word.lower(),\"NaN\") == \"NaN\":\n",
    "        lst = nltk.stem.LancasterStemmer()\n",
    "        if model.vocab.get(lst.stem(word).lower(),\"NaN\") == \"NaN\":\n",
    "            if word[-1] == \"s\" and model.vocab.get(word[:-1].lower(),\"NaN\") != \"NaN\":\n",
    "                return model[word[:-1].lower()]\n",
    "            elif word.lower() in word_vector_dic:\n",
    "                return word_vector_dic[word.lower()]\n",
    "            else:\n",
    "                return model[\"unk\"]\n",
    "        else:\n",
    "            return model[lst.stem(word).lower()]\n",
    "    else:\n",
    "        return model[word.lower()]\n",
    "\n",
    "def word2vector(reviews, model, max_length=1000):\n",
    "    vector_data = np.zeros((len(reviews), max_length,100))\n",
    "    x_length = []\n",
    "    i = 0\n",
    "    for review in reviews:\n",
    "        j = 0 \n",
    "        if len(review) > max_length:\n",
    "            print(\"The length of the reviews is %dwhich is larger than max_length (%d)\"%(len(review),max_length))\n",
    "            print(review)\n",
    "            print(\"-\"*10)\n",
    "        for word in review:\n",
    "            vector_data[i,j] = getvector(word,model)\n",
    "            if str(vector_data[i,j,0])=='nan':\n",
    "                print(word)\n",
    "                break\n",
    "            j += 1\n",
    "        x_length.append(j)\n",
    "        i += 1\n",
    "    return vector_data, np.asarray(x_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f25d91d5dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f25d91d5dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "rnn/last (?, 64)\n",
      "fclayer/hidden (?, 576)\n",
      "rnn_output/y_rnn (?, 41)\n",
      "combine_output/y (?, 41)\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.variable_scope('input'):\n",
    "        x_rnn = tf.placeholder(tf.float32, [None, max_length, word2vector_len], name=\"x_rnn\")\n",
    "        x_naive_bayes = tf.placeholder(tf.float32, [None, n_classes], name=\"x_naive_bayes\")\n",
    "        x_length = tf.placeholder(tf.int32, [None], name=\"x_length\")\n",
    "        y_ = tf.placeholder(tf.int64, [None],  name=\"y_\")\n",
    "\n",
    "        #RNN的初始化状态，全设为零。注意state是与input保持一致，接下来会有concat操作，所以这里要有batch的维度。即每个样本都要有隐层状态\n",
    "        init_state = tf.zeros([batch_size, state_size], name=\"init_state\")\n",
    "\n",
    "    with tf.variable_scope('rnn'):\n",
    "        #定义rnn_cell的权重参数，\n",
    "        #cell = tf.contrib.rnn.BasicRNNCell(state_size, name = \"cell\")\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(state_size, forget_bias = 0, name = \"cell\")\n",
    "        #rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_stateu\n",
    "        rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, inputs = x_rnn, \n",
    "                                                     dtype=tf.float32,\n",
    "                                                     #initial_state=init_state,\n",
    "                                                     sequence_length = x_length)\n",
    "        #print(\"rnn/final_state\", final_state)\n",
    "        #print(\"rnn/rnn_outputs\", rnn_outputs)\n",
    "        last = rnn_outputs[:, -1, :]\n",
    "        print(\"rnn/last\",last.shape)\n",
    "\n",
    "    with tf.variable_scope('fclayer'):\n",
    "        w1 = tf.Variable(tf.random_uniform([state_size, int(state_size*9)],0,1.0),name=\"w1\")\n",
    "        b1= tf.Variable(tf.zeros([int(state_size*9)]),name=\"b1\")\n",
    "        hidden = tf.sigmoid(tf.add(tf.matmul(last, w1), b1),name=\"hidden\")\n",
    "        print(\"fclayer/hidden\", hidden.shape)\n",
    "\n",
    "    with tf.variable_scope('rnn_output'):\n",
    "        w2 = tf.Variable(tf.random_uniform([int(state_size*9), n_classes],0,1.0),name=\"w2\")\n",
    "        b2= tf.Variable(tf.zeros([n_classes]),name=\"b2\")\n",
    "        y_rnn_logits = tf.add(tf.matmul(hidden, w2), b2,name=\"y_rnn_logits\")\n",
    "        y_rnn = tf.nn.softmax(y_rnn_logits, name=\"y_rnn\")\n",
    "        print(\"rnn_output/y_rnn\", y_rnn.shape)\n",
    "\n",
    "    with tf.variable_scope('combine_output'):   \n",
    "        w = tf.Variable(tf.random_uniform([n_classes*2, n_classes],0,1.0),name=\"w\")\n",
    "        b= tf.Variable(tf.zeros([n_classes]),name=\"b\")\n",
    "        y = tf.add(tf.matmul(tf.concat([y_rnn,x_naive_bayes],axis=1), w), b,name=\"y\")\n",
    "        print(\"combine_output/y\",y.shape)\n",
    "\n",
    "    with tf.variable_scope('loss'):\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,\n",
    "                                                                                      labels=y_, \n",
    "                                                                                      name=\"cross_entropy_loss\"))\n",
    "        #cross_entropy_3 = tf.cast(tf.one_hot(y_-1,n_classes),tf.float32)*tf.log(tf.nn.softmax(y))\n",
    "    with tf.variable_scope('Prediction'):\n",
    "        prediction = tf.argmax(tf.nn.softmax(y),1, name=\"prediction\")\n",
    "        cal_accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction,y_), tf.float32),name=\"cal_accuracy\")\n",
    "\n",
    "    with tf.variable_scope('Train'):\n",
    "        #train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "        train_step = tf.train.AdamOptimizer(0.0001).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------1000-----------\n",
      "Accuracy at 1000: 6.86%\n",
      "Loss at 1000：3.5719\n",
      "-----------2000-----------\n",
      "Accuracy at 2000: 14.32%\n",
      "Loss at 2000：3.3722\n",
      "-----------3000-----------\n",
      "Accuracy at 3000: 31.02%\n",
      "Loss at 3000：3.2152\n",
      "-----------4000-----------\n",
      "Accuracy at 4000: 36.37%\n",
      "Loss at 4000：3.0811\n",
      "-----------5000-----------\n",
      "Accuracy at 5000: 36.91%\n",
      "Loss at 5000：2.9598\n",
      "-----------6000-----------\n",
      "Accuracy at 6000: 37.70%\n",
      "Loss at 6000：2.8589\n",
      "-----------7000-----------\n",
      "Accuracy at 7000: 38.40%\n",
      "Loss at 7000：2.7624\n",
      "-----------8000-----------\n",
      "Accuracy at 8000: 39.35%\n",
      "Loss at 8000：2.6735\n",
      "-----------9000-----------\n",
      "Accuracy at 9000: 39.85%\n",
      "Loss at 9000：2.5994\n",
      "-----------10000-----------\n",
      "Accuracy at 10000: 40.93%\n",
      "Loss at 10000：2.5169\n",
      "-----------11000-----------\n",
      "Accuracy at 11000: 42.75%\n",
      "Loss at 11000：2.4438\n",
      "-----------12000-----------\n",
      "Accuracy at 12000: 44.83%\n",
      "Loss at 12000：2.3804\n",
      "-----------13000-----------\n",
      "Accuracy at 13000: 47.40%\n",
      "Loss at 13000：2.3053\n",
      "-----------14000-----------\n",
      "Accuracy at 14000: 48.59%\n",
      "Loss at 14000：2.2423\n",
      "-----------15000-----------\n",
      "Accuracy at 15000: 49.73%\n",
      "Loss at 15000：2.1829\n",
      "-----------16000-----------\n",
      "Accuracy at 16000: 50.20%\n",
      "Loss at 16000：2.1218\n",
      "-----------17000-----------\n",
      "Accuracy at 17000: 51.78%\n",
      "Loss at 17000：2.0621\n",
      "-----------18000-----------\n",
      "Accuracy at 18000: 53.57%\n",
      "Loss at 18000：2.0089\n",
      "-----------19000-----------\n",
      "Accuracy at 19000: 55.29%\n",
      "Loss at 19000：1.9529\n",
      "-----------20000-----------\n",
      "Accuracy at 20000: 55.76%\n",
      "Loss at 20000：1.9047\n",
      "-----------21000-----------\n",
      "Accuracy at 21000: 57.10%\n",
      "Loss at 21000：1.8552\n",
      "-----------22000-----------\n",
      "Accuracy at 22000: 59.36%\n",
      "Loss at 22000：1.8037\n",
      "-----------23000-----------\n",
      "Accuracy at 23000: 60.48%\n",
      "Loss at 23000：1.7710\n",
      "-----------24000-----------\n",
      "Accuracy at 24000: 62.02%\n",
      "Loss at 24000：1.7207\n",
      "-----------25000-----------\n",
      "Accuracy at 25000: 63.32%\n",
      "Loss at 25000：1.6790\n",
      "-----------26000-----------\n",
      "Accuracy at 26000: 64.00%\n",
      "Loss at 26000：1.6526\n",
      "-----------27000-----------\n",
      "Accuracy at 27000: 66.18%\n",
      "Loss at 27000：1.6035\n",
      "-----------28000-----------\n",
      "Accuracy at 28000: 67.63%\n",
      "Loss at 28000：1.5692\n",
      "-----------29000-----------\n",
      "Accuracy at 29000: 68.14%\n",
      "Loss at 29000：1.5486\n",
      "-----------30000-----------\n",
      "Accuracy at 30000: 69.38%\n",
      "Loss at 30000：1.5038\n",
      "-----------31000-----------\n",
      "Accuracy at 31000: 70.60%\n",
      "Loss at 31000：1.4766\n",
      "-----------32000-----------\n",
      "Accuracy at 32000: 70.88%\n",
      "Loss at 32000：1.4537\n",
      "-----------33000-----------\n",
      "Accuracy at 33000: 71.74%\n",
      "Loss at 33000：1.4254\n",
      "-----------34000-----------\n",
      "Accuracy at 34000: 72.88%\n",
      "Loss at 34000：1.3993\n",
      "-----------35000-----------\n",
      "Accuracy at 35000: 73.48%\n",
      "Loss at 35000：1.3757\n",
      "-----------36000-----------\n",
      "Accuracy at 36000: 73.78%\n",
      "Loss at 36000：1.3501\n",
      "-----------37000-----------\n",
      "Accuracy at 37000: 73.75%\n",
      "Loss at 37000：1.3391\n",
      "-----------38000-----------\n",
      "Accuracy at 38000: 74.16%\n",
      "Loss at 38000：1.3104\n",
      "-----------39000-----------\n",
      "Accuracy at 39000: 74.33%\n",
      "Loss at 39000：1.2890\n",
      "-----------40000-----------\n",
      "Accuracy at 40000: 74.30%\n",
      "Loss at 40000：1.2831\n",
      "-----------41000-----------\n",
      "Accuracy at 41000: 74.67%\n",
      "Loss at 41000：1.2531\n",
      "-----------42000-----------\n",
      "Accuracy at 42000: 74.64%\n",
      "Loss at 42000：1.2419\n",
      "-----------43000-----------\n",
      "Accuracy at 43000: 74.40%\n",
      "Loss at 43000：1.2381\n",
      "-----------44000-----------\n",
      "Accuracy at 44000: 75.21%\n",
      "Loss at 44000：1.2079\n",
      "-----------45000-----------\n",
      "Accuracy at 45000: 75.18%\n",
      "Loss at 45000：1.1989\n",
      "-----------46000-----------\n",
      "Accuracy at 46000: 75.22%\n",
      "Loss at 46000：1.1927\n",
      "-----------47000-----------\n",
      "Accuracy at 47000: 75.47%\n",
      "Loss at 47000：1.1736\n",
      "-----------48000-----------\n",
      "Accuracy at 48000: 75.54%\n",
      "Loss at 48000：1.1635\n",
      "-----------49000-----------\n",
      "Accuracy at 49000: 75.65%\n",
      "Loss at 49000：1.1565\n",
      "-----------50000-----------\n",
      "Accuracy at 50000: 75.40%\n",
      "Loss at 50000：1.1481\n",
      "-----------51000-----------\n",
      "Accuracy at 51000: 75.63%\n",
      "Loss at 51000：1.1365\n",
      "-----------52000-----------\n",
      "Accuracy at 52000: 75.82%\n",
      "Loss at 52000：1.1256\n",
      "-----------53000-----------\n",
      "Accuracy at 53000: 75.56%\n",
      "Loss at 53000：1.1200\n",
      "-----------54000-----------\n",
      "Accuracy at 54000: 75.52%\n",
      "Loss at 54000：1.1201\n",
      "-----------55000-----------\n",
      "Accuracy at 55000: 75.78%\n",
      "Loss at 55000：1.1013\n",
      "-----------56000-----------\n",
      "Accuracy at 56000: 75.60%\n",
      "Loss at 56000：1.1012\n",
      "-----------57000-----------\n",
      "Accuracy at 57000: 75.55%\n",
      "Loss at 57000：1.0997\n",
      "-----------58000-----------\n",
      "Accuracy at 58000: 75.93%\n",
      "Loss at 58000：1.0785\n",
      "-----------59000-----------\n",
      "Accuracy at 59000: 75.71%\n",
      "Loss at 59000：1.0837\n",
      "-----------60000-----------\n",
      "Accuracy at 60000: 75.51%\n",
      "Loss at 60000：1.0852\n",
      "-----------61000-----------\n",
      "Accuracy at 61000: 75.88%\n",
      "Loss at 61000：1.0652\n",
      "-----------62000-----------\n",
      "Accuracy at 62000: 75.88%\n",
      "Loss at 62000：1.0663\n",
      "-----------63000-----------\n",
      "Accuracy at 63000: 75.66%\n",
      "Loss at 63000：1.0657\n",
      "-----------64000-----------\n",
      "Accuracy at 64000: 75.66%\n",
      "Loss at 64000：1.0584\n",
      "-----------65000-----------\n",
      "Accuracy at 65000: 75.83%\n",
      "Loss at 65000：1.0549\n",
      "-----------66000-----------\n",
      "Accuracy at 66000: 75.82%\n",
      "Loss at 66000：1.0510\n",
      "-----------67000-----------\n",
      "Accuracy at 67000: 75.63%\n",
      "Loss at 67000：1.0495\n",
      "-----------68000-----------\n",
      "Accuracy at 68000: 75.73%\n",
      "Loss at 68000：1.0477\n",
      "-----------69000-----------\n",
      "Accuracy at 69000: 75.82%\n",
      "Loss at 69000：1.0417\n",
      "-----------70000-----------\n",
      "Accuracy at 70000: 75.67%\n",
      "Loss at 70000：1.0378\n",
      "-----------71000-----------\n",
      "Accuracy at 71000: 75.72%\n",
      "Loss at 71000：1.0420\n",
      "-----------72000-----------\n",
      "Accuracy at 72000: 75.79%\n",
      "Loss at 72000：1.0311\n",
      "-----------73000-----------\n",
      "Accuracy at 73000: 75.76%\n",
      "Loss at 73000：1.0319\n",
      "-----------74000-----------\n",
      "Accuracy at 74000: 75.47%\n",
      "Loss at 74000：1.0403\n",
      "-----------75000-----------\n",
      "Accuracy at 75000: 75.90%\n",
      "Loss at 75000：1.0206\n",
      "-----------76000-----------\n",
      "Accuracy at 76000: 75.68%\n",
      "Loss at 76000：1.0275\n",
      "-----------77000-----------\n",
      "Accuracy at 77000: 75.54%\n",
      "Loss at 77000：1.0328\n",
      "-----------78000-----------\n",
      "Accuracy at 78000: 75.87%\n",
      "Loss at 78000：1.0152\n",
      "-----------79000-----------\n",
      "Accuracy at 79000: 75.83%\n",
      "Loss at 79000：1.0200\n",
      "-----------80000-----------\n",
      "Accuracy at 80000: 75.65%\n",
      "Loss at 80000：1.0195\n",
      "-----------81000-----------\n",
      "Accuracy at 81000: 75.63%\n",
      "Loss at 81000：1.0189\n",
      "-----------82000-----------\n",
      "Accuracy at 82000: 75.73%\n",
      "Loss at 82000：1.0188\n",
      "-----------83000-----------\n",
      "Accuracy at 83000: 75.79%\n",
      "Loss at 83000：1.0119\n",
      "-----------84000-----------\n",
      "Accuracy at 84000: 75.66%\n",
      "Loss at 84000：1.0141\n",
      "-----------85000-----------\n",
      "Accuracy at 85000: 75.60%\n",
      "Loss at 85000：1.0194\n",
      "-----------86000-----------\n",
      "Accuracy at 86000: 75.80%\n",
      "Loss at 86000：1.0083\n",
      "-----------87000-----------\n",
      "Accuracy at 87000: 75.65%\n",
      "Loss at 87000：1.0093\n",
      "-----------88000-----------\n",
      "Accuracy at 88000: 75.65%\n",
      "Loss at 88000：1.0152\n",
      "-----------89000-----------\n",
      "Accuracy at 89000: 75.77%\n",
      "Loss at 89000：1.0041\n",
      "-----------90000-----------\n",
      "Accuracy at 90000: 75.73%\n",
      "Loss at 90000：1.0070\n",
      "-----------91000-----------\n",
      "Accuracy at 91000: 75.42%\n",
      "Loss at 91000：1.0186\n",
      "-----------92000-----------\n",
      "Accuracy at 92000: 75.82%\n",
      "Loss at 92000：0.9990\n",
      "-----------93000-----------\n",
      "Accuracy at 93000: 75.73%\n",
      "Loss at 93000：1.0056\n",
      "-----------94000-----------\n",
      "Accuracy at 94000: 75.58%\n",
      "Loss at 94000：1.0087\n",
      "-----------95000-----------\n",
      "Accuracy at 95000: 75.73%\n",
      "Loss at 95000：0.9996\n",
      "-----------96000-----------\n",
      "Accuracy at 96000: 75.81%\n",
      "Loss at 96000：1.0014\n",
      "-----------97000-----------\n",
      "Accuracy at 97000: 75.72%\n",
      "Loss at 97000：1.0016\n",
      "-----------98000-----------\n",
      "Accuracy at 98000: 75.49%\n",
      "Loss at 98000：1.0053\n",
      "-----------99000-----------\n",
      "Accuracy at 99000: 75.71%\n",
      "Loss at 99000：1.0022\n",
      "-----------100000-----------\n",
      "Accuracy at 100000: 75.84%\n",
      "Loss at 100000：0.9953\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    saver = tf.train.Saver(max_to_keep = 1)\n",
    "    max_output_count = 0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        coord = tf.train.Coordinator()\n",
    "        if CONTINUE != 0:\n",
    "            model_file=tf.train.latest_checkpoint(ckpt_dir)\n",
    "            saver.restore(sess,model_file)\n",
    "        threads = tf.train.start_queue_runners(sess, coord)\n",
    "        try:\n",
    "            i = start_step\n",
    "            avg_accuracy = 0\n",
    "            avg_loss = 0\n",
    "            j = 0\n",
    "            while i < 100000:    \n",
    "                batch_data, batch_label = data.batch_train_set() \n",
    "                batch_data_vec, data_length = word2vector(batch_data, model,max_length)\n",
    "                naive_bayes_input = rp.naive_bayes_model(batch_data,word_matrix, word_list)\n",
    "                y, loss, accuracy, _ = sess.run([prediction, cross_entropy, cal_accuracy, train_step],\n",
    "                                                feed_dict={x_rnn:batch_data_vec, \n",
    "                                                           x_naive_bayes:naive_bayes_input,\n",
    "                                                           y_:batch_label, \n",
    "                                                           x_length:data_length})\n",
    "\n",
    "                avg_accuracy += accuracy\n",
    "                avg_loss += loss\n",
    "                i += 1\n",
    "                #print(\".\",end=\"\")\n",
    "                if i%1000 == 0:\n",
    "                    print(\"-----------%d-----------\"%i)\n",
    "                    print(\"Accuracy at %d: %.2f%%\"%(i,avg_accuracy/10))\n",
    "                    print(\"Loss at %d：%.4f\"%(i, avg_loss/1000))\n",
    "                    saver.save(sess,ckpt_dir+'news_category',global_step=i)\n",
    "                    log.add_log('train_accuracy',i, avg_accuracy/1000)\n",
    "                    log.add_log('train_loss',i, avg_loss/1000)\n",
    "                    log.SaveToFile() \n",
    "                    avg_accuracy = 0\n",
    "                    avg_loss = 0\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('done')\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "            print('quit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model for the test dataset\n",
    "Output the accuracy and the loss of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 50.59 %\n",
      "Test set loss: 2.04 \n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        model_file=tf.train.latest_checkpoint(ckpt_dir)\n",
    "        saver.restore(sess,model_file)\n",
    "        total_accuracy = 0\n",
    "        total_loss = 0\n",
    "        #print(int(data.test_size/data.batch_size)+1)\n",
    "        for i in range(int(data.test_size/data.batch_size)+1):\n",
    "            batch_data, batch_label = data.batch_test_set()\n",
    "            batch_data_vec, data_length = word2vector(batch_data, model,max_length)\n",
    "            naive_bayes_input = rp.naive_bayes_model(batch_data,word_matrix, word_list)\n",
    "            y, loss, accuracy = sess.run([prediction, cross_entropy, cal_accuracy],\n",
    "                                                feed_dict={x_rnn:batch_data_vec, \n",
    "                                                           x_naive_bayes:naive_bayes_input,\n",
    "                                                           y_:batch_label, \n",
    "                                                           x_length:data_length})\n",
    "\n",
    "            total_accuracy += accuracy\n",
    "            total_loss += loss\n",
    "            #print(i,accuracy,loss)\n",
    "\n",
    "        total_accuracy = total_accuracy/i\n",
    "        total_loss = total_loss/i\n",
    "        print(\"Test set accuracy: %.2f %%\"%(total_accuracy*100))\n",
    "        print(\"Test set loss: %.2f \"%(total_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model for the train dataset\n",
    "Output the accuracy and the loss based on the whole train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    model_file=tf.train.latest_checkpoint(ckpt_dir)\n",
    "    saver.restore(sess,model_file)\n",
    "    total_accuracy = 0\n",
    "    total_loss = 0\n",
    "    #print(int(data.test_size/data.batch_size)+1)\n",
    "    for i in range(int(data.train_size/data.batch_size)+1):\n",
    "        batch_data, batch_label = data.batch_train_set()  \n",
    "        batch_data_vec, data_length = word2vector(batch_data, model,max_length)\n",
    "        naive_bayes_input = rp.naive_bayes_model(batch_data,word_matrix, word_list)\n",
    "        y, loss, accuracy = sess.run([prediction, cross_entropy, cal_accuracy],\n",
    "                                            feed_dict={x_rnn:batch_data_vec, \n",
    "                                                       x_naive_bayes:naive_bayes_input,\n",
    "                                                       y_:batch_label, \n",
    "                                                       x_length:data_length})\n",
    "        \n",
    "        total_accuracy += accuracy\n",
    "        total_loss += loss\n",
    "        #print(i,accuracy,loss)\n",
    "        \n",
    "    total_accuracy = total_accuracy/i\n",
    "    total_loss = total_loss/i\n",
    "    print(\"Train set accuracy: %.2f %%\"%(total_accuracy*100))\n",
    "    print(\"Train set loss: %.2f \"%(total_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train my own word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def distinct_words(corpus):\n",
    "    \"\"\" Determine a list of distinct words for the corpus.\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "        Return:\n",
    "            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python 'sorted' function)\n",
    "            num_corpus_words (integer): number of distinct words across the corpus\n",
    "    \"\"\"\n",
    "    corpus_words = []\n",
    "    num_corpus_words = -1\n",
    "    \n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    flat_corpus = [y for x in corpus for y in x]\n",
    "    for x in flat_corpus: \n",
    "        if x not in corpus_words: \n",
    "            corpus_words.append(x)\n",
    "    corpus_words.sort()\n",
    "    num_corpus_words = len(corpus_words)\n",
    "    #print(corpus_words)\n",
    "    # ------------------\n",
    "\n",
    "    return corpus_words, num_corpus_words\n",
    "\n",
    "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
    "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
    "    \n",
    "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
    "              number of co-occurring words.\n",
    "              \n",
    "              For example, if we take the document \"START All that glitters is not gold END\" with window size of 4,\n",
    "              \"All\" will co-occur with \"START\", \"that\", \"glitters\", \"is\", and \"not\".\n",
    "    \n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "            window_size (int): size of context window\n",
    "        Return:\n",
    "            M (numpy matrix of shape (number of corpus words, number of corpus words)): \n",
    "                Co-occurence matrix of word counts. \n",
    "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
    "            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
    "    \"\"\"\n",
    "    words, num_words = distinct_words(corpus)\n",
    "    print('Total %d distinct words in the corpus.'%num_words)\n",
    "    M = None\n",
    "    word2Ind = {}\n",
    "    \n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    M = [[0 for i in range(num_words)] for j in range (num_words)]\n",
    "    index = 0\n",
    "    for w in words:\n",
    "        word2Ind[w]=index\n",
    "        index += 1\n",
    "    for s in corpus:\n",
    "        pos = 0\n",
    "        for c in s:\n",
    "            substr = []\n",
    "            for i in range(1,window_size+1):                \n",
    "                if pos-i >= 0 and s[pos-i] not in substr:\n",
    "                    substr.append(s[pos-i])\n",
    "                if pos+i <len(s) and s[pos+i] not in substr:           \n",
    "                    substr.append(s[pos+i])\n",
    "            for n in substr:\n",
    "                M[word2Ind[c]][word2Ind[n]] += 1\n",
    "            pos += 1\n",
    "    # ------------------\n",
    "    print(\"M shape\", len(M), len(M[0]))\n",
    "    M = np.array(M)\n",
    "    return M, word2Ind\n",
    "\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "corpus = []\n",
    "\n",
    "def reduce_to_k_dim(M, k=2):\n",
    "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
    "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
    "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "    \n",
    "        Params:\n",
    "            M (numpy matrix of shape (number of corpus words, number of corpus words)): co-occurence matrix of word counts\n",
    "            k (int): embedding size of each word after dimension reduction\n",
    "        Return:\n",
    "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
    "                    In terms of the SVD from math class, this actually returns U * S\n",
    "    \"\"\"    \n",
    "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
    "    M_reduced = None\n",
    "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
    "    \n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    svd = TruncatedSVD(n_components=k)\n",
    "    M_reduced=svd.fit_transform(M)\n",
    "    # ------------------\n",
    "\n",
    "    print(\"Done.\")\n",
    "    return M_reduced\n",
    "\n",
    "for title in data.train_data:\n",
    "    corpus.append([START_TOKEN] + title + [END_TOKEN])\n",
    "matrix, word2ind = compute_co_occurrence_matrix(corpus, window_size=2)\n",
    "print(matrix[0])\n",
    "print(len(word2ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3-tensorflow-gpu] *",
   "language": "python",
   "name": "conda-env-python3-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
