{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Combined end-to-end training model for NLP classify problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import threading\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "sys.path.append('../../common/')\n",
    "\n",
    "from train_log import train_log\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\" \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.FATAL)\n",
    "\n",
    "FILE_NAME = 'News_Category_Dataset_v2.json'\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = datapath('/home/ubuntu/Notebooks/GloVe/glove.6B.100d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
    "    \n",
    "import NewsCategoryData as ncd \n",
    "from NewsCategoryData import NewsCategoryTrainTestSet\n",
    "from NewsCategoryData import LABEL_LIST\n",
    "import Run_Prediction as rp\n",
    "\n",
    "CONTINUE = 1\n",
    "start_step = 200000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 200847 recorders are read. 180787 train data and 20060 test data.\n",
      "Load the naive bayes word vectors. It takes a few minutes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "state_size = 64\n",
    "max_length = 100\n",
    "n_classes = 41\n",
    "word2vector_len = 100  \n",
    "data = NewsCategoryTrainTestSet(batch_size=batch_size,max_length=max_length)\n",
    "word_matrix_file = \"naive_bayes_word_matrix_ver1.csv\"\n",
    "word_matrix, word_list = rp.read_naive_bayes_word_vector(word_matrix_file)\n",
    "\n",
    "ckpt_dir = './ckpt_save_combine_model/'\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "log_dir = './log_combine_model/'\n",
    "log = train_log(log_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_window(word, title, window_size = 2):\n",
    "    '''\n",
    "    word: is the word you are looking for.\n",
    "    title: is a word list which contains the given word\n",
    "    window_size: decide how much word you are going to return, \n",
    "    for example 2 means only return the left and right words, \n",
    "    3 means return the left 2 and right 2 words. \n",
    "\n",
    "    '''\n",
    "    close_word_list = []\n",
    "    i = 0 \n",
    "    for i in range(len(title)):\n",
    "        if title[i].lower() == word:\n",
    "            if i > 0:\n",
    "                close_word_list.extend(title[max(i-window_size+1,0):i])\n",
    "            if i < len(title)-1:\n",
    "                close_word_list.extend(title[i+1:min(i+window_size,len(title))])\n",
    "    return close_word_list\n",
    "\n",
    "unknown_word = {}\n",
    "for title in data.train_data:\n",
    "    for word in title:\n",
    "        if model.vocab.get(word.lower(),\"NaN\") == \"NaN\":\n",
    "            if word.lower() in unknown_word:\n",
    "                unknown_word[word.lower()].append(title)\n",
    "            else:\n",
    "                unknown_word[word.lower()] = []\n",
    "                unknown_word[word.lower()].append(title)\n",
    "\n",
    "close_word_dict = {}\n",
    "for word in unknown_word:\n",
    "    close_word_dict[word] = []\n",
    "    for title in unknown_word[word]:\n",
    "        close_word_list = find_window(word, title, window_size = 3)\n",
    "        close_word_dict[word].extend(close_word_list)\n",
    "      \n",
    "    \n",
    "word_vector_dic = {}\n",
    "for word in close_word_dict:\n",
    "    word_vector_dic[word] =np.zeros(100)\n",
    "    for close_word in close_word_dict[word]:\n",
    "        if model.vocab.get(close_word.lower(),\"NaN\") != \"NaN\":\n",
    "            word_vector_dic[word] = np.add(word_vector_dic[word],model[close_word.lower()])/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the out-of-vocabury words with the close words in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvector(word, model):\n",
    "    if model.vocab.get(word.lower(),\"NaN\") == \"NaN\":\n",
    "        lst = nltk.stem.LancasterStemmer()\n",
    "        if model.vocab.get(lst.stem(word).lower(),\"NaN\") == \"NaN\":\n",
    "            if word[-1] == \"s\" and model.vocab.get(word[:-1].lower(),\"NaN\") != \"NaN\":\n",
    "                return model[word[:-1].lower()]\n",
    "            elif word.lower() in word_vector_dic:\n",
    "                return word_vector_dic[word.lower()]\n",
    "            else:\n",
    "                return model[\"unk\"]\n",
    "        else:\n",
    "            return model[lst.stem(word).lower()]\n",
    "    else:\n",
    "        return model[word.lower()]\n",
    "\n",
    "def word2vector(reviews, model, max_length=1000):\n",
    "    vector_data = np.zeros((len(reviews), max_length,100))\n",
    "    x_length = []\n",
    "    i = 0\n",
    "    for review in reviews:\n",
    "        j = 0 \n",
    "        if len(review) > max_length:\n",
    "            print(\"The length of the reviews is %dwhich is larger than max_length (%d)\"%(len(review),max_length))\n",
    "            print(review)\n",
    "            print(\"-\"*10)\n",
    "        for word in review:\n",
    "            vector_data[i,j] = getvector(word,model)\n",
    "            if str(vector_data[i,j,0])=='nan':\n",
    "                print(word)\n",
    "                break\n",
    "            j += 1\n",
    "        x_length.append(j)\n",
    "        i += 1\n",
    "    return vector_data, np.asarray(x_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f8a008ce9e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f8a008ce9e8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "rnn/final_state LSTMStateTuple(c=<tf.Tensor 'rnn/rnn/while/Exit_3:0' shape=(?, 64) dtype=float32>, h=<tf.Tensor 'rnn/rnn/while/Exit_4:0' shape=(?, 64) dtype=float32>)\n",
      "fclayer/hidden (?, 576)\n",
      "rnn_output/y_rnn (?, 41)\n",
      "combine_output/y (?, 41)\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.variable_scope('input'):\n",
    "        x_rnn = tf.placeholder(tf.float32, [None, max_length, word2vector_len], name=\"x_rnn\")\n",
    "        x_naive_bayes = tf.placeholder(tf.float32, [None, n_classes], name=\"x_naive_bayes\")\n",
    "        x_length = tf.placeholder(tf.int32, [None], name=\"x_length\")\n",
    "        y_ = tf.placeholder(tf.int64, [None],  name=\"y_\")\n",
    "\n",
    "        #RNN的初始化状态，全设为零。注意state是与input保持一致，接下来会有concat操作，所以这里要有batch的维度。即每个样本都要有隐层状态\n",
    "        init_state = tf.zeros([batch_size, state_size], name=\"init_state\")\n",
    "\n",
    "    with tf.variable_scope('rnn'):\n",
    "        #定义rnn_cell的权重参数，\n",
    "        #cell = tf.contrib.rnn.BasicRNNCell(state_size, name = \"cell\")\n",
    "        cell = tf.contrib.rnn.BasicLSTMCell(state_size, forget_bias = 0, name = \"cell\")\n",
    "        #rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_stateu\n",
    "        rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, inputs = x_rnn, \n",
    "                                                     dtype=tf.float32,\n",
    "                                                     #initial_state=init_state,\n",
    "                                                     sequence_length = x_length)\n",
    "        print(\"rnn/final_state\", final_state)\n",
    "        #print(\"rnn/rnn_outputs\", rnn_outputs)\n",
    "        #last = rnn_outputs[:, -1, :]\n",
    "        #print(\"rnn/last\",last.shape)\n",
    "\n",
    "    with tf.variable_scope('fclayer'):\n",
    "        w1 = tf.Variable(tf.random_uniform([state_size*2, int(state_size*9)],0,1.0),name=\"w1\")\n",
    "        b1= tf.Variable(tf.zeros([int(state_size*9)]),name=\"b1\")\n",
    "        hidden = tf.sigmoid(tf.add(tf.matmul(tf.concat([final_state[0],final_state[1]],axis=1), w1), b1),name=\"hidden\")\n",
    "        print(\"fclayer/hidden\", hidden.shape)\n",
    "\n",
    "    with tf.variable_scope('rnn_output'):\n",
    "        w2 = tf.Variable(tf.random_uniform([int(state_size*9), n_classes],0,1.0),name=\"w2\")\n",
    "        b2= tf.Variable(tf.zeros([n_classes]),name=\"b2\")\n",
    "        y_rnn_logits = tf.add(tf.matmul(hidden, w2), b2,name=\"y_rnn_logits\")\n",
    "        y_rnn = tf.nn.softmax(y_rnn_logits, name=\"y_rnn\")\n",
    "        print(\"rnn_output/y_rnn\", y_rnn.shape)\n",
    "\n",
    "    with tf.variable_scope('combine_output'):   \n",
    "        w = tf.Variable(tf.random_uniform([n_classes*2, n_classes],0,1.0),name=\"w\")\n",
    "        b= tf.Variable(tf.zeros([n_classes]),name=\"b\")\n",
    "        y = tf.add(tf.matmul(tf.concat([y_rnn,x_naive_bayes],axis=1), w), b,name=\"y\")\n",
    "        print(\"combine_output/y\",y.shape)\n",
    "\n",
    "    with tf.variable_scope('loss'):\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,\n",
    "                                                                                      labels=y_, \n",
    "                                                                                      name=\"cross_entropy_loss\"))\n",
    "        #cross_entropy_3 = tf.cast(tf.one_hot(y_-1,n_classes),tf.float32)*tf.log(tf.nn.softmax(y))\n",
    "    with tf.variable_scope('Prediction'):\n",
    "        prediction = tf.argmax(tf.nn.softmax(y),1, name=\"prediction\")\n",
    "        cal_accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction,y_), tf.float32),name=\"cal_accuracy\")\n",
    "\n",
    "    with tf.variable_scope('Train'):\n",
    "        #train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "        train_step = tf.train.AdamOptimizer(0.00000001).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------201000-----------\n",
      "Accuracy at 201000: 76.69%\n",
      "Loss at 201000：0.9161\n",
      "-----------202000-----------\n",
      "Accuracy at 202000: 77.14%\n",
      "Loss at 202000：0.8979\n",
      "-----------203000-----------\n",
      "Accuracy at 203000: 77.00%\n",
      "Loss at 203000：0.9003\n",
      "-----------204000-----------\n",
      "Accuracy at 204000: 76.61%\n",
      "Loss at 204000：0.9204\n",
      "-----------205000-----------\n",
      "Accuracy at 205000: 77.29%\n",
      "Loss at 205000：0.8905\n",
      "-----------206000-----------\n",
      "Accuracy at 206000: 76.89%\n",
      "Loss at 206000：0.9046\n",
      "-----------207000-----------\n",
      "Accuracy at 207000: 76.72%\n",
      "Loss at 207000：0.9142\n",
      "-----------208000-----------\n",
      "Accuracy at 208000: 77.29%\n",
      "Loss at 208000：0.8912\n",
      "-----------209000-----------\n",
      "Accuracy at 209000: 76.82%\n",
      "Loss at 209000：0.9068\n",
      "-----------210000-----------\n",
      "Accuracy at 210000: 76.84%\n",
      "Loss at 210000：0.9103\n",
      "-----------211000-----------\n",
      "Accuracy at 211000: 77.25%\n",
      "Loss at 211000：0.8919\n",
      "-----------212000-----------\n",
      "Accuracy at 212000: 76.86%\n",
      "Loss at 212000：0.9089\n",
      "-----------213000-----------\n",
      "Accuracy at 213000: 76.82%\n",
      "Loss at 213000：0.9078\n",
      "-----------214000-----------\n",
      "Accuracy at 214000: 77.15%\n",
      "Loss at 214000：0.8953\n",
      "-----------215000-----------\n",
      "Accuracy at 215000: 76.80%\n",
      "Loss at 215000：0.9099\n",
      "-----------216000-----------\n",
      "Accuracy at 216000: 77.14%\n",
      "Loss at 216000：0.8973\n",
      "-----------217000-----------\n",
      "Accuracy at 217000: 77.06%\n",
      "Loss at 217000：0.8986\n",
      "-----------218000-----------\n",
      "Accuracy at 218000: 76.70%\n",
      "Loss at 218000：0.9149\n",
      "-----------219000-----------\n",
      "Accuracy at 219000: 77.16%\n",
      "Loss at 219000：0.8952\n",
      "-----------220000-----------\n",
      "Accuracy at 220000: 77.02%\n",
      "Loss at 220000：0.8979\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    saver = tf.train.Saver(max_to_keep = 1)\n",
    "    max_output_count = 0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        coord = tf.train.Coordinator()\n",
    "        if CONTINUE != 0:\n",
    "            model_file=tf.train.latest_checkpoint(ckpt_dir)\n",
    "            saver.restore(sess,model_file)\n",
    "        threads = tf.train.start_queue_runners(sess, coord)\n",
    "        try:\n",
    "            i = start_step\n",
    "            avg_accuracy = 0\n",
    "            avg_loss = 0\n",
    "            j = 0\n",
    "            while i < 220000:    \n",
    "                batch_data, batch_label = data.batch_train_set() \n",
    "                batch_data_vec, data_length = word2vector(batch_data, model,max_length)\n",
    "                naive_bayes_input = rp.naive_bayes_model(batch_data,word_matrix, word_list)\n",
    "                rnn_output_, final_state_, y, loss, accuracy, _ = sess.run([rnn_outputs, final_state, prediction, cross_entropy, cal_accuracy, train_step],\n",
    "                                                feed_dict={x_rnn:batch_data_vec, \n",
    "                                                           x_naive_bayes:naive_bayes_input,\n",
    "                                                           y_:batch_label, \n",
    "                                                           x_length:data_length})\n",
    "\n",
    "                #print()\n",
    "                #print(rnn_output_)\n",
    "                #print(final_state_)\n",
    "                #print()\n",
    "                avg_accuracy += accuracy\n",
    "                avg_loss += loss\n",
    "                i += 1\n",
    "                #print(\".\",end=\"\")\n",
    "                if i%1000 == 0:\n",
    "                    print(\"-----------%d-----------\"%i)\n",
    "                    print(\"Accuracy at %d: %.2f%%\"%(i,avg_accuracy/10))\n",
    "                    print(\"Loss at %d：%.4f\"%(i, avg_loss/1000))\n",
    "                    saver.save(sess,ckpt_dir+'news_category',global_step=i)\n",
    "                    log.add_log('train_accuracy',i, avg_accuracy/1000)\n",
    "                    log.add_log('train_loss',i, avg_loss/1000)\n",
    "                    log.SaveToFile() \n",
    "                    avg_accuracy = 0\n",
    "                    avg_loss = 0\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('done')\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "            print('quit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model for the test dataset\n",
    "Output the accuracy and the loss of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 51.78 %\n",
      "Test set loss: 1.96 \n"
     ]
    }
   ],
   "source": [
    "with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        model_file=tf.train.latest_checkpoint(ckpt_dir)\n",
    "        saver.restore(sess,model_file)\n",
    "        total_accuracy = 0\n",
    "        total_loss = 0\n",
    "        #print(int(data.test_size/data.batch_size)+1)\n",
    "        for i in range(int(data.test_size/data.batch_size)+1):\n",
    "            batch_data, batch_label = data.batch_test_set()\n",
    "            batch_data_vec, data_length = word2vector(batch_data, model,max_length)\n",
    "            naive_bayes_input = rp.naive_bayes_model(batch_data,word_matrix, word_list)\n",
    "            y, loss, accuracy = sess.run([prediction, cross_entropy, cal_accuracy],\n",
    "                                                feed_dict={x_rnn:batch_data_vec, \n",
    "                                                           x_naive_bayes:naive_bayes_input,\n",
    "                                                           y_:batch_label, \n",
    "                                                           x_length:data_length})\n",
    "\n",
    "            total_accuracy += accuracy\n",
    "            total_loss += loss\n",
    "            #print(i,accuracy,loss)\n",
    "\n",
    "        total_accuracy = total_accuracy/i\n",
    "        total_loss = total_loss/i\n",
    "        print(\"Test set accuracy: %.2f %%\"%(total_accuracy*100))\n",
    "        print(\"Test set loss: %.2f \"%(total_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model for the train dataset\n",
    "Output the accuracy and the loss based on the whole train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        model_file=tf.train.latest_checkpoint(ckpt_dir)\n",
    "        saver.restore(sess,model_file)\n",
    "        total_accuracy = 0\n",
    "        total_loss = 0\n",
    "        #print(int(data.test_size/data.batch_size)+1)\n",
    "        for i in range(int(data.train_size/data.batch_size)+1):\n",
    "            batch_data, batch_label = data.batch_train_set()  \n",
    "            batch_data_vec, data_length = word2vector(batch_data, model,max_length)\n",
    "            naive_bayes_input = rp.naive_bayes_model(batch_data,word_matrix, word_list)\n",
    "            y, loss, accuracy = sess.run([prediction, cross_entropy, cal_accuracy],\n",
    "                                                feed_dict={x_rnn:batch_data_vec, \n",
    "                                                           x_naive_bayes:naive_bayes_input,\n",
    "                                                           y_:batch_label, \n",
    "                                                           x_length:data_length})\n",
    "\n",
    "            total_accuracy += accuracy\n",
    "            total_loss += loss\n",
    "            #print(i,accuracy,loss)\n",
    "\n",
    "        total_accuracy = total_accuracy/i\n",
    "        total_loss = total_loss/i\n",
    "        print(\"Train set accuracy: %.2f %%\"%(total_accuracy*100))\n",
    "        print(\"Train set loss: %.2f \"%(total_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train my own word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def distinct_words(corpus):\n",
    "    \"\"\" Determine a list of distinct words for the corpus.\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "        Return:\n",
    "            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python 'sorted' function)\n",
    "            num_corpus_words (integer): number of distinct words across the corpus\n",
    "    \"\"\"\n",
    "    corpus_words = []\n",
    "    num_corpus_words = -1\n",
    "    \n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    flat_corpus = [y for x in corpus for y in x]\n",
    "    for x in flat_corpus: \n",
    "        if x not in corpus_words: \n",
    "            corpus_words.append(x)\n",
    "    corpus_words.sort()\n",
    "    num_corpus_words = len(corpus_words)\n",
    "    #print(corpus_words)\n",
    "    # ------------------\n",
    "\n",
    "    return corpus_words, num_corpus_words\n",
    "\n",
    "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
    "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
    "    \n",
    "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
    "              number of co-occurring words.\n",
    "              \n",
    "              For example, if we take the document \"START All that glitters is not gold END\" with window size of 4,\n",
    "              \"All\" will co-occur with \"START\", \"that\", \"glitters\", \"is\", and \"not\".\n",
    "    \n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "            window_size (int): size of context window\n",
    "        Return:\n",
    "            M (numpy matrix of shape (number of corpus words, number of corpus words)): \n",
    "                Co-occurence matrix of word counts. \n",
    "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
    "            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
    "    \"\"\"\n",
    "    words, num_words = distinct_words(corpus)\n",
    "    print('Total %d distinct words in the corpus.'%num_words)\n",
    "    M = None\n",
    "    word2Ind = {}\n",
    "    \n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    M = [[0 for i in range(num_words)] for j in range (num_words)]\n",
    "    index = 0\n",
    "    for w in words:\n",
    "        word2Ind[w]=index\n",
    "        index += 1\n",
    "    for s in corpus:\n",
    "        pos = 0\n",
    "        for c in s:\n",
    "            substr = []\n",
    "            for i in range(1,window_size+1):                \n",
    "                if pos-i >= 0 and s[pos-i] not in substr:\n",
    "                    substr.append(s[pos-i])\n",
    "                if pos+i <len(s) and s[pos+i] not in substr:           \n",
    "                    substr.append(s[pos+i])\n",
    "            for n in substr:\n",
    "                M[word2Ind[c]][word2Ind[n]] += 1\n",
    "            pos += 1\n",
    "    # ------------------\n",
    "    print(\"M shape\", len(M), len(M[0]))\n",
    "    M = np.array(M)\n",
    "    return M, word2Ind\n",
    "\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "corpus = []\n",
    "\n",
    "def reduce_to_k_dim(M, k=2):\n",
    "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
    "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
    "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "    \n",
    "        Params:\n",
    "            M (numpy matrix of shape (number of corpus words, number of corpus words)): co-occurence matrix of word counts\n",
    "            k (int): embedding size of each word after dimension reduction\n",
    "        Return:\n",
    "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
    "                    In terms of the SVD from math class, this actually returns U * S\n",
    "    \"\"\"    \n",
    "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
    "    M_reduced = None\n",
    "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
    "    \n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    svd = TruncatedSVD(n_components=k)\n",
    "    M_reduced=svd.fit_transform(M)\n",
    "    # ------------------\n",
    "\n",
    "    print(\"Done.\")\n",
    "    return M_reduced\n",
    "\n",
    "for title in data.train_data:\n",
    "    corpus.append([START_TOKEN] + title + [END_TOKEN])\n",
    "matrix, word2ind = compute_co_occurrence_matrix(corpus, window_size=2)\n",
    "print(matrix[0])\n",
    "print(len(word2ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3-tensorflow-gpu] *",
   "language": "python",
   "name": "conda-env-python3-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
