{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Combined end-to-end training model for NLP classify problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import threading\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "sys.path.append('../../common/')\n",
    "\n",
    "from train_log import train_log\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\" \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.FATAL)\n",
    "\n",
    "FILE_NAME = 'News_Category_Dataset_v2.json'\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = datapath('/home/ubuntu/Notebooks/GloVe/glove.6B.100d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
    "    \n",
    "import NewsCategoryData as ncd \n",
    "from NewsCategoryData import NewsCategoryTrainTestSet\n",
    "from NewsCategoryData import LABEL_LIST\n",
    "import Run_Prediction as rp\n",
    "\n",
    "CONTINUE = 0\n",
    "start_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 200847 recorders are read. 180787 train data and 20060 test data.\n",
      "Load the naive bayes word vectors. It takes a few minutes.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "state_size = 64\n",
    "max_length = 100\n",
    "n_classes = 41\n",
    "word2vector_len = 100  \n",
    "data = NewsCategoryTrainTestSet(batch_size=batch_size,max_length=max_length)\n",
    "word_matrix_file = \"naive_bayes_word_matrix_ver1.csv\"\n",
    "word_matrix, word_list = rp.read_naive_bayes_word_vector(word_matrix_file)\n",
    "\n",
    "ckpt_dir = './ckpt_save_combine_model/'\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "log_dir = './log_combine_model/'\n",
    "log = train_log(log_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f5b26ae0dd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f5b26ae0dd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "fclayer/hidden (?, 576)\n",
      "rnn_output/y_rnn (?, 41)\n",
      "combine_output/y (?, 41)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('input'):\n",
    "    x_rnn = tf.placeholder(tf.float32, [None, max_length, word2vector_len], name=\"x_rnn\")\n",
    "    x_naive_bayes = tf.placeholder(tf.float32, [None, n_classes], name=\"x_naive_bayes\")\n",
    "    x_length = tf.placeholder(tf.int32, [None], name=\"x_length\")\n",
    "    y_ = tf.placeholder(tf.int64, [None],  name=\"y_\")\n",
    "    \n",
    "    #RNN的初始化状态，全设为零。注意state是与input保持一致，接下来会有concat操作，所以这里要有batch的维度。即每个样本都要有隐层状态\n",
    "    init_state = tf.zeros([batch_size, state_size], name=\"init_state\")\n",
    "\n",
    "with tf.variable_scope('rnn'):\n",
    "    #定义rnn_cell的权重参数，\n",
    "    cell = tf.contrib.rnn.BasicRNNCell(state_size, name = \"cell\")\n",
    "    #rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_stateu\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, inputs = x_rnn, \n",
    "                                                 dtype=tf.float32,\n",
    "                                                 #initial_state=init_state,\n",
    "                                                 sequence_length = x_length)\n",
    "with tf.variable_scope('fclayer'):\n",
    "    w1 = tf.Variable(tf.random_uniform([state_size, int(state_size*9)],0,1.0),name=\"w1\")\n",
    "    b1= tf.Variable(tf.zeros([int(state_size*9)]),name=\"b1\")\n",
    "    hidden = tf.sigmoid(tf.add(tf.matmul(final_state, w1), b1),name=\"hidden\")\n",
    "    print(\"fclayer/hidden\", hidden.shape)\n",
    "\n",
    "with tf.variable_scope('rnn_output'):\n",
    "    w2 = tf.Variable(tf.random_uniform([int(state_size*9), n_classes],0,1.0),name=\"w2\")\n",
    "    b2= tf.Variable(tf.zeros([n_classes]),name=\"b2\")\n",
    "    y_rnn_logits = tf.add(tf.matmul(hidden, w2), b2,name=\"y_rnn_logits\")\n",
    "    y_rnn = tf.nn.softmax(y_rnn_logits, name=\"y_rnn\")\n",
    "    print(\"rnn_output/y_rnn\", y_rnn.shape)\n",
    "\n",
    "with tf.variable_scope('combine_output'):   \n",
    "    w = tf.Variable(tf.random_uniform([n_classes*2, n_classes],0,1.0),name=\"w\")\n",
    "    b= tf.Variable(tf.zeros([n_classes]),name=\"b\")\n",
    "    y = tf.add(tf.matmul(tf.concat([y_rnn,x_naive_bayes],axis=1), w), b,name=\"y\")\n",
    "    print(\"combine_output/y\",y.shape)\n",
    "            \n",
    "with tf.variable_scope('loss'):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,\n",
    "                                                                                  labels=y_, \n",
    "                                                                                  name=\"cross_entropy_loss\"))\n",
    "    #cross_entropy_3 = tf.cast(tf.one_hot(y_-1,n_classes),tf.float32)*tf.log(tf.nn.softmax(y))\n",
    "with tf.variable_scope('Prediction'):\n",
    "    prediction = tf.argmax(tf.nn.softmax(y),1, name=\"prediction\")\n",
    "    cal_accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction,y_), tf.float32),name=\"cal_accuracy\")\n",
    "    \n",
    "with tf.variable_scope('Train'):\n",
    "    #train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "    train_step = tf.train.AdamOptimizer(0.0001).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------1000-----------\n",
      "Accuracy at 1000: 5.08%\n",
      "Loss at 1000：3.5757\n",
      "-----------2000-----------\n",
      "Accuracy at 2000: 14.90%\n",
      "Loss at 2000：3.3629\n",
      "-----------3000-----------\n",
      "Accuracy at 3000: 29.98%\n",
      "Loss at 3000：3.1922\n",
      "-----------4000-----------\n",
      "Accuracy at 4000: 34.29%\n",
      "Loss at 4000：3.0503\n",
      "-----------5000-----------\n",
      "Accuracy at 5000: 35.05%\n",
      "Loss at 5000：2.9072\n",
      "-----------6000-----------\n",
      "Accuracy at 6000: 35.72%\n",
      "Loss at 6000：2.7801\n",
      "-----------7000-----------\n",
      "Accuracy at 7000: 36.66%\n",
      "Loss at 7000：2.6770\n",
      "-----------8000-----------\n",
      "Accuracy at 8000: 38.65%\n",
      "Loss at 8000：2.5599\n",
      "-----------9000-----------\n",
      "Accuracy at 9000: 40.29%\n",
      "Loss at 9000：2.4629\n",
      "-----------10000-----------\n",
      "Accuracy at 10000: 41.80%\n",
      "Loss at 10000：2.3851\n",
      "-----------11000-----------\n",
      "Accuracy at 11000: 43.78%\n",
      "Loss at 11000：2.2914\n",
      "-----------12000-----------\n",
      "Accuracy at 12000: 45.89%\n",
      "Loss at 12000：2.2087\n",
      "-----------13000-----------\n",
      "Accuracy at 13000: 47.29%\n",
      "Loss at 13000：2.1479\n",
      "-----------14000-----------\n",
      "Accuracy at 14000: 49.94%\n",
      "Loss at 14000：2.0664\n",
      "-----------15000-----------\n",
      "Accuracy at 15000: 51.90%\n",
      "Loss at 15000：2.0029\n",
      "-----------16000-----------\n",
      "Accuracy at 16000: 52.90%\n",
      "Loss at 16000：1.9502\n",
      "-----------17000-----------\n",
      "Accuracy at 17000: 54.41%\n",
      "Loss at 17000：1.8806\n",
      "-----------18000-----------\n",
      "Accuracy at 18000: 55.57%\n",
      "Loss at 18000：1.8336\n",
      "-----------19000-----------\n",
      "Accuracy at 19000: 56.74%\n",
      "Loss at 19000：1.7833\n",
      "-----------20000-----------\n",
      "Accuracy at 20000: 58.35%\n",
      "Loss at 20000：1.7297\n",
      "-----------21000-----------\n",
      "Accuracy at 21000: 60.21%\n",
      "Loss at 21000：1.6958\n",
      "-----------22000-----------\n",
      "Accuracy at 22000: 62.40%\n",
      "Loss at 22000：1.6416\n",
      "-----------23000-----------\n",
      "Accuracy at 23000: 63.13%\n",
      "Loss at 23000：1.6063\n",
      "-----------24000-----------\n",
      "Accuracy at 24000: 64.76%\n",
      "Loss at 24000：1.5773\n",
      "-----------25000-----------\n",
      "Accuracy at 25000: 66.01%\n",
      "Loss at 25000：1.5308\n",
      "-----------26000-----------\n",
      "Accuracy at 26000: 66.62%\n",
      "Loss at 26000：1.4979\n",
      "-----------27000-----------\n",
      "Accuracy at 27000: 67.44%\n",
      "Loss at 27000：1.4767\n",
      "-----------28000-----------\n",
      "Accuracy at 28000: 68.08%\n",
      "Loss at 28000：1.4387\n",
      "-----------29000-----------\n",
      "Accuracy at 29000: 69.07%\n",
      "Loss at 29000：1.4081\n",
      "-----------30000-----------\n",
      "Accuracy at 30000: 69.93%\n",
      "Loss at 30000：1.3912\n",
      "-----------31000-----------\n",
      "Accuracy at 31000: 70.53%\n",
      "Loss at 31000：1.3579\n",
      "-----------32000-----------\n",
      "Accuracy at 32000: 71.19%\n",
      "Loss at 32000：1.3365\n",
      "-----------33000-----------\n",
      "Accuracy at 33000: 71.35%\n",
      "Loss at 33000：1.3214\n",
      "-----------34000-----------\n",
      "Accuracy at 34000: 71.86%\n",
      "Loss at 34000：1.2923\n",
      "-----------35000-----------\n",
      "Accuracy at 35000: 72.50%\n",
      "Loss at 35000：1.2765\n",
      "-----------36000-----------\n",
      "Accuracy at 36000: 72.65%\n",
      "Loss at 36000：1.2582\n",
      "-----------37000-----------\n",
      "Accuracy at 37000: 72.92%\n",
      "Loss at 37000：1.2409\n",
      "-----------38000-----------\n",
      "Accuracy at 38000: 73.49%\n",
      "Loss at 38000：1.2282\n",
      "-----------39000-----------\n",
      "Accuracy at 39000: 73.89%\n",
      "Loss at 39000：1.2044\n",
      "-----------40000-----------\n",
      "Accuracy at 40000: 73.97%\n",
      "Loss at 40000：1.1954\n",
      "-----------41000-----------\n",
      "Accuracy at 41000: 74.25%\n",
      "Loss at 41000：1.1861\n",
      "-----------42000-----------\n",
      "Accuracy at 42000: 74.45%\n",
      "Loss at 42000：1.1668\n",
      "-----------43000-----------\n",
      "Accuracy at 43000: 74.66%\n",
      "Loss at 43000：1.1558\n",
      "-----------44000-----------\n",
      "Accuracy at 44000: 74.77%\n",
      "Loss at 44000：1.1455\n",
      "-----------45000-----------\n",
      "Accuracy at 45000: 74.72%\n",
      "Loss at 45000：1.1361\n",
      "-----------46000-----------\n",
      "Accuracy at 46000: 75.14%\n",
      "Loss at 46000：1.1228\n",
      "-----------47000-----------\n",
      "Accuracy at 47000: 75.06%\n",
      "Loss at 47000：1.1175\n",
      "-----------48000-----------\n",
      "Accuracy at 48000: 75.20%\n",
      "Loss at 48000：1.1046\n",
      "-----------49000-----------\n",
      "Accuracy at 49000: 75.56%\n",
      "Loss at 49000：1.0956\n",
      "-----------50000-----------\n",
      "Accuracy at 50000: 75.40%\n",
      "Loss at 50000：1.0913\n",
      "-----------51000-----------\n",
      "Accuracy at 51000: 75.35%\n",
      "Loss at 51000：1.0842\n",
      "-----------52000-----------\n",
      "Accuracy at 52000: 75.61%\n",
      "Loss at 52000：1.0761\n",
      "-----------53000-----------\n",
      "Accuracy at 53000: 75.73%\n",
      "Loss at 53000：1.0644\n",
      "-----------54000-----------\n",
      "Accuracy at 54000: 75.45%\n",
      "Loss at 54000：1.0675\n",
      "-----------55000-----------\n",
      "Accuracy at 55000: 75.68%\n",
      "Loss at 55000：1.0584\n",
      "-----------56000-----------\n",
      "Accuracy at 56000: 75.83%\n",
      "Loss at 56000：1.0452\n",
      "-----------57000-----------\n",
      "Accuracy at 57000: 75.62%\n",
      "Loss at 57000：1.0476\n",
      "-----------58000-----------\n",
      "Accuracy at 58000: 75.71%\n",
      "Loss at 58000：1.0453\n",
      "-----------59000-----------\n",
      "Accuracy at 59000: 75.78%\n",
      "Loss at 59000：1.0351\n",
      "-----------60000-----------\n",
      "Accuracy at 60000: 75.92%\n",
      "Loss at 60000：1.0299\n",
      "-----------61000-----------\n",
      "Accuracy at 61000: 75.77%\n",
      "Loss at 61000：1.0280\n",
      "-----------62000-----------\n",
      "Accuracy at 62000: 75.81%\n",
      "Loss at 62000：1.0242\n",
      "-----------63000-----------\n",
      "Accuracy at 63000: 76.05%\n",
      "Loss at 63000：1.0177\n",
      "-----------64000-----------\n",
      "Accuracy at 64000: 75.73%\n",
      "Loss at 64000：1.0188\n",
      "-----------65000-----------\n",
      "Accuracy at 65000: 75.91%\n",
      "Loss at 65000：1.0112\n",
      "-----------66000-----------\n",
      "Accuracy at 66000: 76.15%\n",
      "Loss at 66000：1.0075\n",
      "-----------67000-----------\n",
      "Accuracy at 67000: 75.90%\n",
      "Loss at 67000：1.0066\n",
      "-----------68000-----------\n",
      "Accuracy at 68000: 75.91%\n",
      "Loss at 68000：1.0049\n",
      "-----------69000-----------\n",
      "Accuracy at 69000: 76.06%\n",
      "Loss at 69000：1.0008\n",
      "-----------70000-----------\n",
      "Accuracy at 70000: 76.13%\n",
      "Loss at 70000：0.9926\n",
      "-----------71000-----------\n",
      "Accuracy at 71000: 75.92%\n",
      "Loss at 71000：1.0000\n",
      "-----------72000-----------\n",
      "Accuracy at 72000: 76.01%\n",
      "Loss at 72000：0.9945\n",
      "-----------73000-----------\n",
      "Accuracy at 73000: 76.04%\n",
      "Loss at 73000：0.9884\n",
      "-----------74000-----------\n",
      "Accuracy at 74000: 76.13%\n",
      "Loss at 74000：0.9865\n",
      "-----------75000-----------\n",
      "Accuracy at 75000: 75.97%\n",
      "Loss at 75000：0.9905\n",
      "-----------76000-----------\n",
      "Accuracy at 76000: 76.05%\n",
      "Loss at 76000：0.9828\n",
      "-----------77000-----------\n",
      "Accuracy at 77000: 76.27%\n",
      "Loss at 77000：0.9790\n",
      "-----------78000-----------\n",
      "Accuracy at 78000: 76.11%\n",
      "Loss at 78000：0.9783\n",
      "-----------79000-----------\n",
      "Accuracy at 79000: 76.03%\n",
      "Loss at 79000：0.9796\n",
      "-----------80000-----------\n",
      "Accuracy at 80000: 76.25%\n",
      "Loss at 80000：0.9780\n",
      "-----------81000-----------\n",
      "Accuracy at 81000: 76.06%\n",
      "Loss at 81000：0.9755\n",
      "-----------82000-----------\n",
      "Accuracy at 82000: 76.05%\n",
      "Loss at 82000：0.9747\n",
      "-----------83000-----------\n",
      "Accuracy at 83000: 76.39%\n",
      "Loss at 83000：0.9677\n",
      "-----------84000-----------\n",
      "Accuracy at 84000: 76.13%\n",
      "Loss at 84000：0.9695\n",
      "-----------85000-----------\n",
      "Accuracy at 85000: 76.04%\n",
      "Loss at 85000：0.9733\n",
      "-----------86000-----------\n",
      "Accuracy at 86000: 76.25%\n",
      "Loss at 86000：0.9674\n",
      "-----------87000-----------\n",
      "Accuracy at 87000: 76.30%\n",
      "Loss at 87000：0.9601\n",
      "-----------88000-----------\n",
      "Accuracy at 88000: 76.09%\n",
      "Loss at 88000：0.9704\n",
      "-----------89000-----------\n",
      "Accuracy at 89000: 76.19%\n",
      "Loss at 89000：0.9646\n",
      "-----------90000-----------\n",
      "Accuracy at 90000: 76.22%\n",
      "Loss at 90000：0.9601\n",
      "-----------91000-----------\n",
      "Accuracy at 91000: 76.23%\n",
      "Loss at 91000：0.9613\n",
      "-----------92000-----------\n",
      "Accuracy at 92000: 76.26%\n",
      "Loss at 92000：0.9604\n",
      "-----------93000-----------\n",
      "Accuracy at 93000: 76.12%\n",
      "Loss at 93000：0.9590\n",
      "-----------94000-----------\n",
      "Accuracy at 94000: 76.33%\n",
      "Loss at 94000：0.9592\n",
      "-----------95000-----------\n",
      "Accuracy at 95000: 76.26%\n",
      "Loss at 95000：0.9550\n",
      "-----------96000-----------\n",
      "Accuracy at 96000: 76.13%\n",
      "Loss at 96000：0.9565\n",
      "-----------97000-----------\n",
      "Accuracy at 97000: 76.50%\n",
      "Loss at 97000：0.9523\n",
      "-----------98000-----------\n",
      "Accuracy at 98000: 76.23%\n",
      "Loss at 98000：0.9549\n",
      "-----------99000-----------\n",
      "Accuracy at 99000: 76.15%\n",
      "Loss at 99000：0.9545\n",
      "-----------100000-----------\n",
      "Accuracy at 100000: 76.44%\n",
      "Loss at 100000：0.9503\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(max_to_keep = 1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    if CONTINUE != 0:\n",
    "        model_file=tf.train.latest_checkpoint(ckpt_dir)\n",
    "        saver.restore(sess,model_file)\n",
    "    threads = tf.train.start_queue_runners(sess, coord)\n",
    "    try:\n",
    "        i = start_step\n",
    "        avg_accuracy = 0\n",
    "        avg_loss = 0\n",
    "        while i < 100000:    \n",
    "            batch_data, batch_label = data.batch_train_set()  \n",
    "            batch_data_vec, data_length = ncd.word2vector(batch_data, model,max_length)\n",
    "            naive_bayes_input = rp.naive_bayes_model(batch_data,word_matrix, word_list)\n",
    "            y, loss, accuracy, _ = sess.run([prediction, cross_entropy, cal_accuracy, train_step],\n",
    "                                            feed_dict={x_rnn:batch_data_vec, \n",
    "                                                       x_naive_bayes:naive_bayes_input,\n",
    "                                                       y_:batch_label, \n",
    "                                                       x_length:data_length})\n",
    "         \n",
    "            avg_accuracy += accuracy\n",
    "            avg_loss += loss\n",
    "            i += 1\n",
    "            #print(\".\",end=\"\")\n",
    "            if i%1000 == 0:\n",
    "                print(\"-----------%d-----------\"%i)\n",
    "                print(\"Accuracy at %d: %.2f%%\"%(i,avg_accuracy/10))\n",
    "                print(\"Loss at %d：%.4f\"%(i, avg_loss/1000))\n",
    "                saver.save(sess,ckpt_dir+'news_category',global_step=i)\n",
    "                log.add_log('train_accuracy',i, avg_accuracy/1000)\n",
    "                log.add_log('train_loss',i, avg_loss/1000)\n",
    "                log.SaveToFile() \n",
    "                avg_accuracy = 0\n",
    "                avg_loss = 0\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('done')\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        print('quit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy: 51.31 %\n",
      "Test set loss: 1.98 \n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    model_file=tf.train.latest_checkpoint(ckpt_dir)\n",
    "    saver.restore(sess,model_file)\n",
    "    total_accuracy = 0\n",
    "    total_loss = 0\n",
    "    #print(int(data.test_size/data.batch_size)+1)\n",
    "    for i in range(int(data.test_size/data.batch_size)+1):\n",
    "        batch_data, batch_label = data.batch_test_set()  \n",
    "        batch_data_vec, data_length = ncd.word2vector(batch_data, model,max_length)\n",
    "        naive_bayes_input = rp.naive_bayes_model(batch_data,word_matrix, word_list)\n",
    "        y, loss, accuracy = sess.run([prediction, cross_entropy, cal_accuracy],\n",
    "                                            feed_dict={x_rnn:batch_data_vec, \n",
    "                                                       x_naive_bayes:naive_bayes_input,\n",
    "                                                       y_:batch_label, \n",
    "                                                       x_length:data_length})\n",
    "        \n",
    "        total_accuracy += accuracy\n",
    "        total_loss += loss\n",
    "        #print(i,accuracy,loss)\n",
    "        \n",
    "    total_accuracy = total_accuracy/i\n",
    "    total_loss = total_loss/i\n",
    "    print(\"Test set accuracy: %.2f %%\"%(total_accuracy*100))\n",
    "    print(\"Test set loss: %.2f \"%(total_loss))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3-tensorflow-gpu] *",
   "language": "python",
   "name": "conda-env-python3-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
