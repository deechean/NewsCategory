{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Combined end-to-end training model for NLP classify problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import threading\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "sys.path.append('../common/')\n",
    "\n",
    "from train_log import train_log\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\" \n",
    "#os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import tensorflow as tf\n",
    "#tf.logging.set_verbosity(tf.logging.FATAL)\n",
    "\n",
    "#Datafile, json format\n",
    "FILE_NAME = 'News_Category_Dataset_v2.json'\n",
    "\n",
    "#import glove2word2vec from gensim\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = datapath('C:/Users/Wangdi/Documents/Notebook/GloVe/glove.6B.100d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "#print(word2vec_glove_file)\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
    "    \n",
    "import NewsCategoryData as ncd \n",
    "from NewsCategoryData import NewsCategoryTrainTestSet\n",
    "from NewsCategoryData import LABEL_LIST\n",
    "import Run_Prediction as rp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the out-of-vocabury words with the close words in the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_length = 100\n",
    "data = NewsCategoryTrainTestSet(batch_size=batch_size,max_length=max_length)\n",
    "word_matrix_file = \"naive_bayes_word_matrix_ver1.csv\"\n",
    "word_matrix, word_list = rp.read_naive_bayes_word_vector(word_matrix_file)\n",
    "\n",
    "def find_window(word, title, window_size = 2):\n",
    "    '''\n",
    "    word: is the word you are looking for.\n",
    "    title: is a word list which contains the given word\n",
    "    window_size: decide how much word you are going to return, \n",
    "    for example 2 means only return the left and right words, \n",
    "    3 means return the left 2 and right 2 words. \n",
    "\n",
    "    '''\n",
    "    close_word_list = []\n",
    "    i = 0 \n",
    "    for i in range(len(title)):\n",
    "        if title[i].lower() == word:\n",
    "            if i > 0:\n",
    "                close_word_list.extend(title[max(i-window_size+1,0):i])\n",
    "            if i < len(title)-1:\n",
    "                close_word_list.extend(title[i+1:min(i+window_size,len(title))])\n",
    "    return close_word_list\n",
    "\n",
    "unknown_word = {}\n",
    "for title in data.train_data:\n",
    "    for word in title:\n",
    "        if model.vocab.get(word.lower(),\"NaN\") == \"NaN\":\n",
    "            if word.lower() in unknown_word:\n",
    "                unknown_word[word.lower()].append(title)\n",
    "            else:\n",
    "                unknown_word[word.lower()] = []\n",
    "                unknown_word[word.lower()].append(title)\n",
    "\n",
    "close_word_dict = {}\n",
    "for word in unknown_word:\n",
    "    close_word_dict[word] = []\n",
    "    for title in unknown_word[word]:\n",
    "        close_word_list = find_window(word, title, window_size = 3)\n",
    "        close_word_dict[word].extend(close_word_list)\n",
    "      \n",
    "    \n",
    "word_vector_dic = {}\n",
    "for word in close_word_dict:\n",
    "    word_vector_dic[word] =np.zeros(100)\n",
    "    for close_word in close_word_dict[word]:\n",
    "        if model.vocab.get(close_word.lower(),\"NaN\") != \"NaN\":\n",
    "            word_vector_dic[word] = np.add(word_vector_dic[word],model[close_word.lower()])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getvector(word, model):\n",
    "    if model.vocab.get(word.lower(),\"NaN\") == \"NaN\":\n",
    "        lst = nltk.stem.LancasterStemmer()\n",
    "        if model.vocab.get(lst.stem(word).lower(),\"NaN\") == \"NaN\":\n",
    "            if word[-1] == \"s\" and model.vocab.get(word[:-1].lower(),\"NaN\") != \"NaN\":\n",
    "                return model[word[:-1].lower()]\n",
    "            elif word.lower() in word_vector_dic:\n",
    "                return word_vector_dic[word.lower()]\n",
    "            else:\n",
    "                return model[\"unk\"]\n",
    "        else:\n",
    "            return model[lst.stem(word).lower()]\n",
    "    else:\n",
    "        return model[word.lower()]\n",
    "\n",
    "def word2vector(reviews, model, max_length=1000):\n",
    "    vector_data = np.zeros((len(reviews), max_length,100))\n",
    "    x_length = []\n",
    "    i = 0\n",
    "    for review in reviews:\n",
    "        j = 0 \n",
    "        if len(review) > max_length:\n",
    "            print(\"The length of the reviews is %dwhich is larger than max_length (%d)\"%(len(review),max_length))\n",
    "            print(review)\n",
    "            print(\"-\"*10)\n",
    "        for word in review:\n",
    "            vector_data[i,j] = getvector(word,model)\n",
    "            if str(vector_data[i,j,0])=='nan':\n",
    "                print(word)\n",
    "                break\n",
    "            j += 1\n",
    "        x_length.append(j)\n",
    "        i += 1\n",
    "    return vector_data, np.asarray(x_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_size = 100\n",
    "n_classes = 41\n",
    "word2vector_len = 100  \n",
    "\n",
    "timestamp = \"{0:%Y-%m-%dT%H-%M-%S/}\".format(datetime.now())\n",
    "\n",
    "ckpt_dir = './ckpt_save_combine_model/'\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "log_dir = './log_combine_model/log_'+timestamp\n",
    "#log_dir = 'log_2021-08-26T17-12-04'\n",
    "#log = train_log(log_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define a function to build a single-layer recursive neural network\n",
    "\n",
    "x1->x2->x3->final_state->y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_simple_rnn(x_rnn, x_length, y_, batch_size, state_size):\n",
    "    '''create a simple rnn structure\n",
    "    input: \n",
    "    x_rnn: tensor placeholder for x input\n",
    "    x_length: tensor placeholder for the dynamic_rnn paramter sequence_length \n",
    "    y_: tensor placeholder for y lable\n",
    "    batch_size: define the batch size \n",
    "    return\n",
    "    y: tenfor for logit output\n",
    "    '''\n",
    "    with tf.variable_scope('rnn'):\n",
    "        init_state = tf.zeros([batch_size, state_size], name=\"init_state\")     \n",
    "        cell = tf.contrib.rnn.BasicRNNCell(state_size, name = \"cell\")          \n",
    "        rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, inputs = x_rnn, \n",
    "                                                     dtype=tf.float32,\n",
    "                                                     initial_state=init_state,\n",
    "                                                     sequence_length = x_length)\n",
    "        print(\"rnn/rnn_outputs\", rnn_outputs.shape)\n",
    "        print(\"rnn/final_state\", final_state.shape)\n",
    "\n",
    "    with tf.variable_scope('fclayer'):\n",
    "        w1 = tf.Variable(tf.random_uniform([state_size, int(state_size*7)],0,1.0),name=\"w1\")\n",
    "        b1= tf.Variable(tf.zeros([int(state_size*7)]),name=\"b1\")\n",
    "        hidden = tf.tanh(tf.add(tf.matmul(final_state,w1), b1),name=\"hidden\")\n",
    "        print(\"fclayer/hidden\", hidden.shape)\n",
    "\n",
    "    with tf.variable_scope('rnn_output'):\n",
    "        w2 = tf.Variable(tf.random_uniform([int(state_size*7), n_classes],0,1.0),name=\"w2\")\n",
    "        b2= tf.Variable(tf.zeros([n_classes]),name=\"b2\")\n",
    "        y = tf.add(tf.matmul(hidden, w2), b2,name=\"y\")\n",
    "        print(\"rnn_output/y\", y.shape)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to build a single-layer recursive neural network and use all the rnn output for cassify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_simple_rnn_2(x_rnn, x_length, y_, batch_size, state_size):\n",
    "    '''create a simple rnn structure\n",
    "    input: \n",
    "    x_rnn: tensor placeholder for x input\n",
    "    x_length: tensor placeholder for the dynamic_rnn paramter sequence_length \n",
    "    y_: tensor placeholder for y lable\n",
    "    batch_size: define the batch size \n",
    "    return\n",
    "    y: tenfor for logit output\n",
    "    '''\n",
    "    num_of_neurals = 3\n",
    "    with tf.variable_scope('rnn'):\n",
    "        #init_state = tf.zeros([batch_size, state_size], name=\"init_state\")     \n",
    "        cell = tf.contrib.rnn.BasicRNNCell(state_size, name = \"cell\")          \n",
    "        rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, inputs = x_rnn, \n",
    "                                                     dtype=tf.float32,\n",
    "                                                     #initial_state=init_state,\n",
    "                                                     sequence_length = x_length)\n",
    "        #rnn_outputs = tf.contrib.layers.flatten(rnn_outputs)\n",
    "        rnn_outputs=tf.reshape(rnn_outputs,[rnn_outputs.shape[0],rnn_outputs.shape[1],rnn_outputs.shape[2],1])\n",
    "        print(\"rnn/rnn_outputs\", rnn_outputs.shape)\n",
    "        print(\"rnn/final_state\", final_state.shape)\n",
    "\n",
    "    with tf.variable_scope('fclayer'):\n",
    "        stddev = 1. / tf.sqrt(tf.cast(32, tf.float32))\n",
    "        kernel = tf.Variable(tf.random_uniform([8, 8, rnn_outputs.get_shape()[-1], 32], \n",
    "                                               minval=-stddev, maxval=stddev),\n",
    "                             name='kernel')\n",
    "        hidden = tf.nn.conv2d(rnn_outputs, kernel, [1, 1, 1, 1], padding='VALID', name='conv')\n",
    "        bias = tf.zeros([32], name = 'bias')\n",
    "        hidden = tf.nn.bias_add(hidden, bias)\n",
    "        hidden = tf.contrib.layers.flatten(hidden)\n",
    "        print(\"fclayer/hidden\", hidden.shape)\n",
    "\n",
    "    with tf.variable_scope('rnn_output'):\n",
    "        w2 = tf.Variable(tf.random_uniform([hidden.shape[1], n_classes],0,1.0),name=\"w2\")\n",
    "        b2= tf.Variable(tf.zeros([n_classes]),name=\"b2\")\n",
    "        y = tf.add(tf.matmul(hidden, w2), b2,name=\"y\")\n",
    "        print(\"rnn_output/y\", y.shape)\n",
    "        tf.summary.histogram(\"rnn_output/w2\",w2)\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define a function to build a multi-layer recursive neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_multilayer_rnn(x_rnn, x_length, y_, batch_size, state_size):\n",
    "    '''create a multi-layer rnn structure\n",
    "    input: \n",
    "    x_rnn: tensor placeholder for x input\n",
    "    x_length: tensor placeholder for the dynamic_rnn paramter sequence_length \n",
    "    y_: tensor placeholder for y lable\n",
    "    batch_size: define the batch size \n",
    "    return\n",
    "    y: tenfor for logit output\n",
    "    '''\n",
    "    with tf.variable_scope('rnn'):\n",
    "        #init_state = tf.zeros([batch_size, state_size], name=\"init_state\")\n",
    "        cells = [tf.contrib.rnn.BasicLSTMCell(state_size, forget_bias = 0, name = \"cell\") for _ in range(3)]\n",
    "        staked_rnn_cells = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "        rnn_outputs, final_state = tf.nn.dynamic_rnn(staked_rnn_cells,\n",
    "                                                     inputs = x_rnn, \n",
    "                                                     dtype=tf.float32,\n",
    "                                                     initial_state=init_state,\n",
    "                                                     sequence_length = x_length)\n",
    "        #rnn_outputs = tf.transpose(rnn_outputs,[1,0,2])\n",
    "        print(\"rnn/rnn_outputs\", rnn_outputs.shape)\n",
    "        print(\"rnn/final_state\", final_state)\n",
    "\n",
    "    with tf.variable_scope('fclayer'):\n",
    "        w1 = tf.Variable(tf.random_uniform([state_size*2, int(state_size*9)],0,1.0),name=\"w1\")\n",
    "        b1= tf.Variable(tf.zeros([int(state_size*9)]),name=\"b1\")\n",
    "        hidden = tf.nn.relu(tf.add(tf.matmul(tf.concat([final_state[2].c,\n",
    "                                                    final_state[2].h],axis=1),w1), b1),name=\"hidden\")\n",
    "        print(\"fclayer/hidden\", hidden.shape)\n",
    "\n",
    "    with tf.variable_scope('rnn_output'):\n",
    "        w2 = tf.Variable(tf.random_uniform([int(state_size*9), n_classes],0,1.0),name=\"w2\")\n",
    "        b2= tf.Variable(tf.zeros([n_classes]),name=\"b2\")\n",
    "        y = tf.add(tf.matmul(hidden, w2), b2,name=\"y\")\n",
    "        print(\"rnn_output/y\", y.shape)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the recursive neural network structure based on the rnn_type input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rnn_type = 'Single-layer-2' \n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.variable_scope('input'):\n",
    "        x_rnn = tf.placeholder(tf.float32, [batch_size, max_length, word2vector_len], name=\"x_rnn\")\n",
    "        x_length = tf.placeholder(tf.int32, [batch_size], name=\"x_length\")\n",
    "        y_ = tf.placeholder(tf.int64, [batch_size],  name=\"y_\")\n",
    "        print('input/x_rnn',x_rnn.shape)\n",
    "        print('input/y_',y_.shape)    \n",
    "    \n",
    "    if rnn_type == 'Single-layer' :\n",
    "        y = create_simple_rnn(x_rnn, x_length, y_, batch_size, state_size)\n",
    "    elif rnn_type == 'Multi-layer' :\n",
    "        y =create_multilayer_rnn(x_rnn, x_length, y_, batch_size, state_size)\n",
    "    elif rnn_type == 'Single-layer-2' :\n",
    "        y = create_simple_rnn_2(x_rnn, x_length, y_, batch_size, state_size)\n",
    "        \n",
    "    with tf.variable_scope('loss'):\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,\n",
    "                                                                                      labels=y_, \n",
    "                                                                                      name=\"cross_entropy_loss\"))\n",
    "        tf.summary.scalar('loss/cross_entropy', cross_entropy)\n",
    "\n",
    "    with tf.variable_scope('Prediction'):\n",
    "        prediction = tf.argmax(tf.nn.softmax(y),1, name=\"prediction\")\n",
    "        cal_accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction,y_), tf.float32),name=\"cal_accuracy\")\n",
    "        tf.summary.scalar('Prediction/cal_accuracy', cal_accuracy)\n",
    "\n",
    "    with tf.variable_scope('Train'):\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(0.00001)\n",
    "        optimizer = tf.train.AdamOptimizer(0.0001)\n",
    "        \n",
    "        #grad_w_1 = tf.reduce_mean(tf.gradients(cross_entropy, [graph.get_tensor_by_name('fclayer/w1:0')]))\n",
    "        grad_w_2 =  tf.reduce_mean(tf.gradients(cross_entropy, [graph.get_tensor_by_name('rnn_output/w2:0')]))\n",
    "        train_step = optimizer.minimize(cross_entropy)\n",
    "        #tf.summary.scalar('Train/grad_w_1',grad_w_1)\n",
    "        tf.summary.scalar('Train/grad_w_2',grad_w_2)\n",
    "\n",
    "    merged = tf.summary.merge_all()\n",
    "    train_writer = tf.summary.FileWriter(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Use multi-layer rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "if rnn_type == 'Single-layer RNN': \n",
    "    with graph.as_default():\n",
    "        with tf.variable_scope('input'):\n",
    "            x_rnn = tf.placeholder(tf.float32, [None, max_length, word2vector_len], name=\"x_rnn\")\n",
    "            x_naive_bayes = tf.placeholder(tf.float32, [None, n_classes], name=\"x_naive_bayes\")\n",
    "            x_length = tf.placeholder(tf.int32, [None], name=\"x_length\")\n",
    "            y_ = tf.placeholder(tf.int64, [None],  name=\"y_\")\n",
    "\n",
    "            #RNN的初始化状态，全设为零。注意state是与input保持一致，接下来会有concat操作，所以这里要有batch的维度。即每个样本都要有隐层状态\n",
    "            init_state = tf.zeros([batch_size, state_size], name=\"init_state\")\n",
    "\n",
    "        with tf.variable_scope('rnn'):\n",
    "            #定义rnn_cell的权重参数，\n",
    "            #cell = tf.contrib.rnn.BasicRNNCell(state_size, name = \"cell\")\n",
    "            cell = tf.contrib.rnn.BasicLSTMCell(state_size, forget_bias = 0, name = \"cell\")\n",
    "            rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, inputs = x_rnn, \n",
    "                                                         dtype=tf.float32,\n",
    "                                                         #initial_state=init_state,\n",
    "                                                         sequence_length = x_length)\n",
    "            print(\"rnn/final_state\", final_state)\n",
    "\n",
    "        with tf.variable_scope('fclayer'):\n",
    "            w1 = tf.Variable(tf.random_uniform([state_size*2, int(state_size*9)],0,1.0),name=\"w1\")\n",
    "            b1= tf.Variable(tf.zeros([int(state_size*9)]),name=\"b1\")\n",
    "            hidden = tf.tanh(tf.add(tf.matmul(tf.concat([final_state[0],final_state[1]],axis=1), w1), b1),name=\"hidden\")\n",
    "            print(\"fclayer/hidden\", hidden.shape)\n",
    "            tf.summary.histogram('w1', w1)\n",
    "\n",
    "        with tf.variable_scope('rnn_output'):\n",
    "            w2 = tf.Variable(tf.random_uniform([int(state_size*9), n_classes],0,1.0),name=\"w2\")\n",
    "            b2= tf.Variable(tf.zeros([n_classes]),name=\"b2\")\n",
    "            y_rnn_logits = tf.add(tf.matmul(hidden, w2), b2,name=\"y_rnn_logits\")\n",
    "            y_rnn = tf.nn.softmax(y_rnn_logits, name=\"y_rnn\")\n",
    "            print(\"rnn_output/y_rnn\", y_rnn.shape)\n",
    "            tf.summary.histogram('w2', w2)\n",
    "\n",
    "        with tf.variable_scope('combine_output'):   \n",
    "            w = tf.Variable(tf.random_uniform([n_classes*2, n_classes],0,1.0),name=\"w\")\n",
    "            b= tf.Variable(tf.zeros([n_classes]),name=\"b\")\n",
    "            y = tf.add(tf.matmul(tf.concat([y_rnn,x_naive_bayes],axis=1), w), b,name=\"y\")\n",
    "            print(\"combine_output/y\",y.shape)\n",
    "\n",
    "        with tf.variable_scope('loss'):\n",
    "            cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,\n",
    "                                                                                          labels=y_, \n",
    "                                                                                          name=\"cross_entropy_loss\"))\n",
    "            tf.summary.scalar('cross_entropy_loss', cross_entropy)\n",
    "            #cross_entropy_3 = tf.cast(tf.one_hot(y_-1,n_classes),tf.float32)*tf.log(tf.nn.softmax(y))\n",
    "            \n",
    "        with tf.variable_scope('Prediction'):\n",
    "            prediction = tf.argmax(tf.nn.softmax(y),1, name=\"prediction\")\n",
    "            cal_accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction,y_), tf.float32),name=\"cal_accuracy\")\n",
    "            tf.summary.scalar('cal_accuracy', cal_accuracy)\n",
    "\n",
    "        with tf.variable_scope('Train'):\n",
    "            optimizer = tf.train.GradientDescentOptimizer(0.0001)\n",
    "            #optimizer = tf.train.AdamOptimizer(0.00001)\n",
    "            grad_w_1 = tf.reduce_mean(tf.gradients(cross_entropy, [w1]))\n",
    "            grad_w_2 =  tf.reduce_mean(tf.gradients(cross_entropy, [w2]))\n",
    "            train_step = optimizer.minimize(cross_entropy)\n",
    "            tf.summary.scalar('Train/grad_w_1',grad_w_1)\n",
    "            tf.summary.scalar('Train/grad_w_2',grad_w_2)\n",
    "            \n",
    "        merged = tf.summary.merge_all()\n",
    "        train_writer = tf.summary.FileWriter(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model for the test dataset\n",
    "Output the accuracy and the loss of the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CONTINUE = 0\n",
    "start_step = 0\n",
    "\n",
    "with graph.as_default():\n",
    "    saver = tf.train.Saver(max_to_keep = 1)\n",
    "    max_output_count = 0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        coord = tf.train.Coordinator()\n",
    "        if CONTINUE != 0:\n",
    "            model_file=tf.train.latest_checkpoint(ckpt_dir)\n",
    "            saver.restore(sess,model_file)\n",
    "        threads = tf.train.start_queue_runners(sess, coord)\n",
    "        try:\n",
    "            i = start_step\n",
    "            avg_accuracy = 0\n",
    "            avg_loss = 0\n",
    "            j = 0\n",
    "            while i < 50000:    \n",
    "                batch_data, batch_label = data.batch_train_set() \n",
    "                batch_data_vec, data_length = word2vector(batch_data, model,max_length)\n",
    "                naive_bayes_input = rp.naive_bayes_model(batch_data,word_matrix, word_list)\n",
    "                summary, prediction_result, label , loss, accuracy, _ = sess.run([merged, prediction, y_, cross_entropy, cal_accuracy, train_step],\n",
    "                                                    feed_dict={x_rnn:batch_data_vec, \n",
    "                                                               #x_naive_bayes:naive_bayes_input,\n",
    "                                                               y_:batch_label, \n",
    "                                                               x_length:data_length})\n",
    "                \n",
    "\n",
    "                avg_accuracy += accuracy\n",
    "                avg_loss += loss\n",
    "                i += 1\n",
    "                #print(\".\",end=\"\")\n",
    "                if i%1000 == 0:\n",
    "                    print(\"-----------%d-----------\"%i)\n",
    "                    print(\"Accuracy at %d: %.2f%%\"%(i,avg_accuracy/10))\n",
    "                    print(\"Loss at %d：%.4f\"%(i, avg_loss/1000))\n",
    "                    print('----------label---------')\n",
    "                    print(label)\n",
    "                    print('----------prediction---------')\n",
    "                    print(prediction_result)\n",
    "                    saver.save(sess,ckpt_dir+'news_category',global_step=i)\n",
    "                    train_writer.add_summary(summary, i)\n",
    "                    avg_accuracy = 0\n",
    "                    avg_loss = 0\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('done')\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "            print('quit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    saver = tf.train.Saver(max_to_keep = 1)\n",
    "    with tf.Session() as sess:\n",
    "        model_file=tf.train.latest_checkpoint(ckpt_dir)\n",
    "        saver.restore(sess,model_file)\n",
    "        total_accuracy = 0\n",
    "        total_loss = 0\n",
    "        #print(int(data.test_size/data.batch_size)+1)\n",
    "        for i in range(int(data.test_size/data.batch_size)+1):\n",
    "            batch_data, batch_label = data.batch_test_set()\n",
    "            batch_data_vec, data_length = word2vector(batch_data, model,max_length)\n",
    "            #naive_bayes_input = rp.naive_bayes_model(batch_data,word_matrix, word_list)\n",
    "            y, loss, accuracy = sess.run([prediction, cross_entropy, cal_accuracy],\n",
    "                                                feed_dict={x_rnn:batch_data_vec, \n",
    "                                                           #x_naive_bayes:naive_bayes_input,\n",
    "                                                           y_:batch_label, \n",
    "                                                           x_length:data_length})\n",
    "\n",
    "            total_accuracy += accuracy\n",
    "            total_loss += loss\n",
    "            #print(i,accuracy,loss)\n",
    "\n",
    "        total_accuracy = total_accuracy/i\n",
    "        total_loss = total_loss/i\n",
    "        print(\"Test set accuracy: %.2f %%\"%(total_accuracy*100))\n",
    "        print(\"Test set loss: %.2f \"%(total_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model for the train dataset\n",
    "Output the accuracy and the loss based on the whole train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    with tf.Session() as sess:\n",
    "        model_file=tf.train.latest_checkpoint(ckpt_dir)\n",
    "        saver.restore(sess,model_file)\n",
    "        total_accuracy = 0\n",
    "        total_loss = 0\n",
    "        #print(int(data.test_size/data.batch_size)+1)\n",
    "        for i in range(int(data.train_size/data.batch_size)+1):\n",
    "            batch_data, batch_label = data.batch_train_set()  \n",
    "            batch_data_vec, data_length = word2vector(batch_data, model,max_length)\n",
    "            naive_bayes_input = rp.naive_bayes_model(batch_data,word_matrix, word_list)\n",
    "            y, loss, accuracy = sess.run([prediction, cross_entropy, cal_accuracy],\n",
    "                                                feed_dict={x_rnn:batch_data_vec, \n",
    "                                                           x_naive_bayes:naive_bayes_input,\n",
    "                                                           y_:batch_label, \n",
    "                                                           x_length:data_length})\n",
    "\n",
    "            total_accuracy += accuracy\n",
    "            total_loss += loss\n",
    "            #print(i,accuracy,loss)\n",
    "\n",
    "        total_accuracy = total_accuracy/i\n",
    "        total_loss = total_loss/i\n",
    "        print(\"Train set accuracy: %.2f %%\"%(total_accuracy*100))\n",
    "        print(\"Train set loss: %.2f \"%(total_loss))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train my own word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def distinct_words(corpus):\n",
    "    \"\"\" Determine a list of distinct words for the corpus.\n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "        Return:\n",
    "            corpus_words (list of strings): list of distinct words across the corpus, sorted (using python 'sorted' function)\n",
    "            num_corpus_words (integer): number of distinct words across the corpus\n",
    "    \"\"\"\n",
    "    corpus_words = []\n",
    "    num_corpus_words = -1\n",
    "    \n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    flat_corpus = [y for x in corpus for y in x]\n",
    "    for x in flat_corpus: \n",
    "        if x not in corpus_words: \n",
    "            corpus_words.append(x)\n",
    "    corpus_words.sort()\n",
    "    num_corpus_words = len(corpus_words)\n",
    "    #print(corpus_words)\n",
    "    # ------------------\n",
    "\n",
    "    return corpus_words, num_corpus_words\n",
    "\n",
    "def compute_co_occurrence_matrix(corpus, window_size=4):\n",
    "    \"\"\" Compute co-occurrence matrix for the given corpus and window_size (default of 4).\n",
    "    \n",
    "        Note: Each word in a document should be at the center of a window. Words near edges will have a smaller\n",
    "              number of co-occurring words.\n",
    "              \n",
    "              For example, if we take the document \"START All that glitters is not gold END\" with window size of 4,\n",
    "              \"All\" will co-occur with \"START\", \"that\", \"glitters\", \"is\", and \"not\".\n",
    "    \n",
    "        Params:\n",
    "            corpus (list of list of strings): corpus of documents\n",
    "            window_size (int): size of context window\n",
    "        Return:\n",
    "            M (numpy matrix of shape (number of corpus words, number of corpus words)): \n",
    "                Co-occurence matrix of word counts. \n",
    "                The ordering of the words in the rows/columns should be the same as the ordering of the words given by the distinct_words function.\n",
    "            word2Ind (dict): dictionary that maps word to index (i.e. row/column number) for matrix M.\n",
    "    \"\"\"\n",
    "    words, num_words = distinct_words(corpus)\n",
    "    print('Total %d distinct words in the corpus.'%num_words)\n",
    "    M = None\n",
    "    word2Ind = {}\n",
    "    \n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    M = [[0 for i in range(num_words)] for j in range (num_words)]\n",
    "    index = 0\n",
    "    for w in words:\n",
    "        word2Ind[w]=index\n",
    "        index += 1\n",
    "    for s in corpus:\n",
    "        pos = 0\n",
    "        for c in s:\n",
    "            substr = []\n",
    "            for i in range(1,window_size+1):                \n",
    "                if pos-i >= 0 and s[pos-i] not in substr:\n",
    "                    substr.append(s[pos-i])\n",
    "                if pos+i <len(s) and s[pos+i] not in substr:           \n",
    "                    substr.append(s[pos+i])\n",
    "            for n in substr:\n",
    "                M[word2Ind[c]][word2Ind[n]] += 1\n",
    "            pos += 1\n",
    "    # ------------------\n",
    "    print(\"M shape\", len(M), len(M[0]))\n",
    "    M = np.array(M)\n",
    "    return M, word2Ind\n",
    "\n",
    "START_TOKEN = '<START>'\n",
    "END_TOKEN = '<END>'\n",
    "corpus = []\n",
    "\n",
    "def reduce_to_k_dim(M, k=2):\n",
    "    \"\"\" Reduce a co-occurence count matrix of dimensionality (num_corpus_words, num_corpus_words)\n",
    "        to a matrix of dimensionality (num_corpus_words, k) using the following SVD function from Scikit-Learn:\n",
    "            - http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "    \n",
    "        Params:\n",
    "            M (numpy matrix of shape (number of corpus words, number of corpus words)): co-occurence matrix of word counts\n",
    "            k (int): embedding size of each word after dimension reduction\n",
    "        Return:\n",
    "            M_reduced (numpy matrix of shape (number of corpus words, k)): matrix of k-dimensioal word embeddings.\n",
    "                    In terms of the SVD from math class, this actually returns U * S\n",
    "    \"\"\"    \n",
    "    n_iters = 10     # Use this parameter in your call to `TruncatedSVD`\n",
    "    M_reduced = None\n",
    "    print(\"Running Truncated SVD over %i words...\" % (M.shape[0]))\n",
    "    \n",
    "    # ------------------\n",
    "    # Write your implementation here.\n",
    "    svd = TruncatedSVD(n_components=k)\n",
    "    M_reduced=svd.fit_transform(M)\n",
    "    # ------------------\n",
    "\n",
    "    print(\"Done.\")\n",
    "    return M_reduced\n",
    "\n",
    "for title in data.train_data:\n",
    "    corpus.append([START_TOKEN] + title + [END_TOKEN])\n",
    "matrix, word2ind = compute_co_occurrence_matrix(corpus, window_size=2)\n",
    "print(matrix[0])\n",
    "print(len(word2ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "gensim.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
