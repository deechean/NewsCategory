{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Naive Bayes to predict the News Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\wangdi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import csv\n",
    "\n",
    "from NewsCategoryData import NewsCategory\n",
    "from NewsCategoryData import NewsCategoryTrainTestSet\n",
    "from NewsCategoryData import LABEL_LIST\n",
    "\n",
    "jason_data_file_name  = \"News_Category_Dataset_v2.json\"\n",
    "train_test_dataset_file_name = \"news_cat_train_test_data.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1st step: count the words by category\n",
    "\n",
    "Below function will return a dictionary which can tell us how many times the word is used in each category the given dataset. \n",
    "\n",
    "A sample of the data:\n",
    "\n",
    "\"there\":[5.1, 65.1, 8.1, 13.1, 136.1, 11.1, 12.1, 15.1, 16.1, 24.1, 18.1, 20.1, 32.1, 11.1, 8.1, 11.1, 8.1, 7.1, 3.1, 5.1, 13.1, 6.1, 12.1, 8.1, 10.1, 40.1, 4.1, 0.1, 15.1, 5.1, 5.1, 73.1, 37.1, 1.1, 31.1, 10.1, 7.1, 15.1, 6.1, 2.1, 0.1]\n",
    "\n",
    "## 2nd step: generate word matrix and word list\n",
    "Based on the word dictionary created in 1st step, this step will calculate the propobility of each words used in each category. It will return a numpy array (count of words x number of class). The word list is a list of words whose order is the same as the numpy array. For example, word_list[0] = \"there\" then the word_matrix[0] is the probability list of \"there\" in 41 category. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words_by_category(data, label, class_num):\n",
    "    word_dict = {}\n",
    "    assert(len(data) == len(label))\n",
    "    max_recorder = len(label)\n",
    "    for i in range(max_recorder):\n",
    "        #print(data[i])\n",
    "        for word in data[i]:\n",
    "            if word.lower() in word_dict:\n",
    "                word_dict[word.lower()][label[i]] += 1\n",
    "            else:\n",
    "                word_dict[word.lower()] = [0.1 for i in range(class_num)]\n",
    "                word_dict[word.lower()][label[i]] += 1\n",
    "    return word_dict\n",
    "\n",
    "def generate_word_matrix(word_dict):\n",
    "    word_list = {}\n",
    "    word_matrix = []\n",
    "    index = 0\n",
    "    for word in word_dict:\n",
    "        word_list[word] = index \n",
    "        word_matrix.append(word_dict[word])\n",
    "        index += 1\n",
    "    word_matrix = np.asarray(word_matrix) \n",
    "    word_matrix = np.divide(word_matrix,word_matrix.sum(axis=0))\n",
    "    return word_matrix, word_list\n",
    "\n",
    "def save_word_matrix(file_name, word_matrix, word_list):\n",
    "    fo = open(file_name, \"w+\")\n",
    "    writer = csv.writer(fo)\n",
    "    for key in word_list:\n",
    "        writer.writerow([key]+list(word_matrix[word_list[key]]))\n",
    "    fo.close()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the jason format raw data to gnerate the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'News_Category_Dataset_v2.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-08bd5809e6be>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnum_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m41\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNewsCategory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mword_count_dic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcount_words_by_category\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_count_dic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Notebook\\NewsCategory\\NewsCategoryData.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filepath, batch_size, max_length, shuffle)\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_recorder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_recorder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Notebook\\NewsCategory\\NewsCategoryData.py\u001b[0m in \u001b[0;36m_read_file\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m             \u001b[0mjsonData\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[0mjsonData\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'News_Category_Dataset_v2.json'"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "max_length = 100\n",
    "num_class = 41\n",
    "data = NewsCategory(batch_size=batch_size,max_length=max_length)\n",
    "word_count_dic = count_words_by_category(data.data,data.label, num_class)\n",
    "print(len(word_count_dic))\n",
    "for item in word_count_dic:   \n",
    "    print(item,word_count_dic[item])\n",
    "    break\n",
    "file_name = \"naive_bayes_word_matrix.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load train and test dataset to generate the word_matrix and save as files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 200847 recorders are read. 180787 train data and 20060 test data.\n",
      "55530\n",
      "there : [5.1, 61.1, 7.1, 13.1, 126.1, 10.1, 12.1, 13.1, 15.1, 22.1, 18.1, 18.1, 32.1, 10.1, 7.1, 9.1, 7.1, 6.1, 2.1, 5.1, 11.1, 5.1, 10.1, 8.1, 10.1, 35.1, 4.1, 0.1, 15.1, 5.1, 5.1, 67.1, 31.1, 1.1, 29.1, 9.1, 7.1, 14.1, 4.1, 2.1, 0.1]\n",
      "(55530, 41)\n",
      "55530\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "max_length = 100\n",
    "num_class = 41\n",
    "data = NewsCategoryTrainTestSet(batch_size=batch_size,max_length=max_length)\n",
    "word_count_dic = count_words_by_category(data.train_data, data.train_label, num_class)\n",
    "print(len(word_count_dic))\n",
    "for item in word_count_dic:   \n",
    "    print(item,\":\", word_count_dic[item])\n",
    "    break\n",
    "word_matrix, word_list = generate_word_matrix(word_count_dic)\n",
    "print(word_matrix.shape)\n",
    "print(len(word_list))\n",
    "\n",
    "file_name = \"naive_bayes_word_matrix_ver1.csv\"\n",
    "save_word_matrix(file_name,word_matrix, word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58834\n",
      "(58834, 41)\n"
     ]
    }
   ],
   "source": [
    "word_matrix_2 = []\n",
    "word_list_2 = {}\n",
    "fo = open(\"naive_bayes_word_matrix.csv\", \"r+\")\n",
    "reader = csv.reader(fo)\n",
    "index = 0\n",
    "for row in reader:\n",
    "    word_list_2[row[0]] = index\n",
    "    index += 1\n",
    "    m=[]\n",
    "    for data in row[1:]:\n",
    "        m.append(float(data))\n",
    "    word_matrix_2.append(m)\n",
    "word_matrix_2 = np.array(word_matrix_2)\n",
    "print(len(word_list_2))\n",
    "print(word_matrix_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy rate = 77.40 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "accuracy = 0\n",
    "icount = 0\n",
    "prediction = []\n",
    "for title in data.data:\n",
    "    y = np.array([1 for i in range(word_matrix.shape[1])], dtype=np.float32)\n",
    "    x = [] \n",
    "    for word in title:\n",
    "        x.append(word_matrix[word_list[word.lower()]])\n",
    "    y = np.asarray(x).prod(axis=0)   \n",
    "    #print(\"y=\",y)\n",
    "    prediction.append(y.argmax())\n",
    "    #print(prediction,data.label[icount])\n",
    "    if y.argmax() == data.label[icount]:\n",
    "        accuracy += 1\n",
    "    icount += 1\n",
    "print(\"Accuracy rate = %.2f %%\"%(accuracy/len(data.label)*100))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
