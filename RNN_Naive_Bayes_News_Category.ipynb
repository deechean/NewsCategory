{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import threading\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "sys.path.append('../../common/')\n",
    "\n",
    "from train_log import train_log\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\" \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.FATAL)\n",
    "\n",
    "FILE_NAME = 'News_Category_Dataset_v2.json'\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = datapath('/home/ubuntu/Notebooks/GloVe/glove.6B.100d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
    "\n",
    "ckpt_dir_cnn = './ckpt_save_8/'\n",
    "ckpt_dir_this = './ckpt_save/'\n",
    "if not os.path.exists(ckpt_dir_this):\n",
    "    os.makedirs(ckpt_dir_this)\n",
    "log_dir = './log/'\n",
    "log = train_log(log_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "CONTINUE = 0\n",
    "start_step = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NewsCategoryData import NewsCategory\n",
    "\n",
    "def getvector(word, model=model):\n",
    "    if model.vocab.get(word.lower(),\"NaN\") == \"NaN\":\n",
    "        lst = nltk.stem.LancasterStemmer()\n",
    "        if model.vocab.get(lst.stem(word).lower(),\"NaN\") == \"NaN\":\n",
    "            if word[-1] == \"s\" and model.vocab.get(word[:-1].lower(),\"NaN\") != \"NaN\":\n",
    "                return model[word[:-1].lower()]\n",
    "            else:\n",
    "                return model[\"unk\"]\n",
    "        else:\n",
    "            return model[lst.stem(word).lower()]\n",
    "    else:\n",
    "        return model[word.lower()]\n",
    "\n",
    "def word2vector(reviews, model=model, max_length=1000):\n",
    "    vector_data = np.zeros((len(reviews), max_length,100))\n",
    "    x_length = []\n",
    "    i = 0\n",
    "    for review in reviews:\n",
    "        j = 0 \n",
    "        if len(review) > max_length:\n",
    "            print(\"The length of the reviews is %dwhich is larger than max_length (%d)\"%(len(review),max_length))\n",
    "            print(review)\n",
    "            print(\"-\"*10)\n",
    "        for word in review:\n",
    "            vector_data[i,j] = getvector(word,model)\n",
    "            if str(vector_data[i,j,0])=='nan':\n",
    "                print(word)\n",
    "                break\n",
    "            j += 1\n",
    "        x_length.append(j)\n",
    "        i += 1\n",
    "    return vector_data, np.asarray(x_length)\n",
    "\n",
    "def getvector2(word, model=model):\n",
    "    if model.vocab.get(word.lower(),\"NaN\") == \"NaN\":\n",
    "        lst = nltk.stem.LancasterStemmer()\n",
    "        if model.vocab.get(lst.stem(word).lower(),\"NaN\") == \"NaN\":\n",
    "            if word[-1] == \"s\" and model.vocab.get(word[:-1].lower(),\"NaN\") != \"NaN\":\n",
    "                #print(word[:-1])\n",
    "                return word[:-1].lower()\n",
    "            else:\n",
    "                return \"unk\"\n",
    "        else:\n",
    "            return lst.stem(word).lower()\n",
    "    else:\n",
    "        return word.lower()\n",
    "batch_size = 64\n",
    "n_classes = 41\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_classes = 41\n",
    "graph_combined = tf.Graph()\n",
    "with graph_combined.as_default():\n",
    "    with tf.variable_scope('Combined_Input'):\n",
    "        x_logits = tf.placeholder(tf.float32, [None, n_classes], name=\"x_logits\")\n",
    "        x_cnn = tf.nn.softmax(x_logits, name = \"x_cnn\")\n",
    "        x_naive_bayes = tf.placeholder(tf.float32, [None,n_classes], name=\"x_naive_bayes\")\n",
    "        y_ = tf.placeholder(tf.int64, [None],  name=\"y_\")\n",
    "\n",
    "    with tf.variable_scope('Combined_Output'):\n",
    "        w = tf.Variable(tf.random_uniform([n_classes*2, n_classes],0,1.0),name=\"w\")\n",
    "        b= tf.Variable(tf.zeros([n_classes]),name=\"b\")\n",
    "        y = tf.add(tf.matmul(tf.concat([x_cnn,x_naive_bayes],axis=1), w), b,name=\"y\")\n",
    "        #print(y.shape)\n",
    "\n",
    "    with tf.variable_scope('Combined_Loss'):\n",
    "        cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,\n",
    "                                                                                      labels=y_, \n",
    "                                                                                      name=\"cross_entropy_loss\"))\n",
    "        #cross_entropy_3 = tf.cast(tf.one_hot(y_-1,n_classes),tf.float32)*tf.log(tf.nn.softmax(y))\n",
    "        \n",
    "    with tf.variable_scope('Combined_Prediction'):\n",
    "        prediction = tf.argmax(tf.nn.softmax(y),1, name=\"prediction\")\n",
    "        cal_accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction,y_), tf.float32),name=\"cal_accuracy\")\n",
    "\n",
    "    with tf.variable_scope('Combined_Train'):\n",
    "        #train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "        train_step = tf.train.AdamOptimizer(0.0001).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_cnn = tf.Graph()\n",
    "with graph_cnn.as_default(): \n",
    "    saver = tf.train.import_meta_graph(ckpt_dir_cnn+'news_category-200000.meta')\n",
    "    input_x = graph_cnn.get_tensor_by_name(\"Input/x:0\")\n",
    "    input_y = graph_cnn.get_tensor_by_name(\"Input/y_:0\")\n",
    "    x_length = graph_cnn.get_tensor_by_name(\"Input/x_length:0\")\n",
    "    output_y = graph_cnn.get_tensor_by_name(\"Output/y:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the naive bayes word vectors. It takes a few minutes.\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "import Run_Prediction as rp\n",
    "max_length = 100\n",
    "\n",
    "cnn_logits_list = []\n",
    "naive_bayes_input_list = []\n",
    "label_list = [] \n",
    "with graph_cnn.as_default(): \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess, coord)\n",
    "\n",
    "        data = NewsCategory(batch_size=batch_size,max_length=max_length)\n",
    "        #load the pretrained cnn model\n",
    "        model_file=tf.train.latest_checkpoint(ckpt_dir_cnn)\n",
    "        saver.restore(sess,model_file)\n",
    "        #load naive bayes model\n",
    "        word_matrix_file = \"naive_bayes_word_matrix.csv\"\n",
    "        word_matrix, word_list = rp.read_naive_bayes_word_vector(word_matrix_file)\n",
    "\n",
    "        try:        \n",
    "            i = start_step\n",
    "            avg_accuracy = 0\n",
    "            avg_loss = 0\n",
    "            while i < int(data.max_recorder/batch_size)+1:    \n",
    "                batch_data, batch_label = data.get_batch_data()  \n",
    "                batch_data_vec, data_length = word2vector(batch_data, model,max_length)\n",
    "                \n",
    "                cnn_logits = sess.run([output_y],\n",
    "                                      feed_dict={input_x:batch_data_vec, \n",
    "                                                 input_y:batch_label, \n",
    "                                                 x_length:data_length})\n",
    "                cnn_logits_list.append(cnn_logits[0])\n",
    "\n",
    "                naive_bayes_input = rp.naive_bayes_model(batch_data,word_matrix, word_list)\n",
    "                naive_bayes_input_list.append(naive_bayes_input)\n",
    "                #print(naive_bayes_input.shape)\n",
    "            \n",
    "                label_list.append(batch_label)\n",
    "                \n",
    "                '''\n",
    "                print('cnn_logits',cnn_logits[0])\n",
    "                if cnn_logits[0].shape != (batch_size,n_classes):\n",
    "                    print(cnn_logits[0])\n",
    "                print('naive_bayes_input', naive_bayes_input)\n",
    "                if np.isnan(naive_bayes_input).any():\n",
    "                    print(batch_data)\n",
    "                '''\n",
    "                i += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('done')\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "            print('quit')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------1000-----------\n",
      "Accuracy at 1000: 3.51%\n",
      "Loss at 1000：3.6749\n",
      "-----------2000-----------\n",
      "Accuracy at 2000: 13.40%\n",
      "Loss at 2000：3.4534\n",
      "-----------3000-----------\n",
      "Accuracy at 3000: 22.24%\n",
      "Loss at 3000：3.2502\n",
      "-----------4000-----------\n",
      "Accuracy at 4000: 34.13%\n",
      "Loss at 4000：3.0539\n",
      "-----------5000-----------\n",
      "Accuracy at 5000: 43.58%\n",
      "Loss at 5000：2.8775\n",
      "-----------6000-----------\n",
      "Accuracy at 6000: 44.74%\n",
      "Loss at 6000：2.7133\n",
      "-----------7000-----------\n",
      "Accuracy at 7000: 45.52%\n",
      "Loss at 7000：2.5576\n",
      "-----------8000-----------\n",
      "Accuracy at 8000: 46.37%\n",
      "Loss at 8000：2.4193\n",
      "-----------9000-----------\n",
      "Accuracy at 9000: 47.39%\n",
      "Loss at 9000：2.2921\n",
      "-----------10000-----------\n",
      "Accuracy at 10000: 48.71%\n",
      "Loss at 10000：2.1732\n",
      "-----------11000-----------\n",
      "Accuracy at 11000: 50.71%\n",
      "Loss at 11000：2.0698\n",
      "-----------12000-----------\n",
      "Accuracy at 12000: 52.34%\n",
      "Loss at 12000：1.9739\n",
      "-----------13000-----------\n",
      "Accuracy at 13000: 55.15%\n",
      "Loss at 13000：1.8744\n",
      "-----------14000-----------\n",
      "Accuracy at 14000: 56.77%\n",
      "Loss at 14000：1.8029\n",
      "-----------15000-----------\n",
      "Accuracy at 15000: 59.03%\n",
      "Loss at 15000：1.7226\n",
      "-----------16000-----------\n",
      "Accuracy at 16000: 61.13%\n",
      "Loss at 16000：1.6485\n",
      "-----------17000-----------\n",
      "Accuracy at 17000: 62.72%\n",
      "Loss at 17000：1.5921\n",
      "-----------18000-----------\n",
      "Accuracy at 18000: 64.86%\n",
      "Loss at 18000：1.5254\n",
      "-----------19000-----------\n",
      "Accuracy at 19000: 66.80%\n",
      "Loss at 19000：1.4711\n",
      "-----------20000-----------\n",
      "Accuracy at 20000: 68.56%\n",
      "Loss at 20000：1.4223\n",
      "-----------21000-----------\n",
      "Accuracy at 21000: 70.33%\n",
      "Loss at 21000：1.3737\n",
      "-----------22000-----------\n",
      "Accuracy at 22000: 72.01%\n",
      "Loss at 22000：1.3292\n",
      "-----------23000-----------\n",
      "Accuracy at 23000: 73.14%\n",
      "Loss at 23000：1.2889\n",
      "-----------24000-----------\n",
      "Accuracy at 24000: 74.11%\n",
      "Loss at 24000：1.2562\n",
      "-----------25000-----------\n",
      "Accuracy at 25000: 75.47%\n",
      "Loss at 25000：1.2125\n",
      "-----------26000-----------\n",
      "Accuracy at 26000: 76.03%\n",
      "Loss at 26000：1.1863\n",
      "-----------27000-----------\n",
      "Accuracy at 27000: 76.40%\n",
      "Loss at 27000：1.1598\n",
      "-----------28000-----------\n",
      "Accuracy at 28000: 77.10%\n",
      "Loss at 28000：1.1265\n",
      "-----------29000-----------\n",
      "Accuracy at 29000: 77.68%\n",
      "Loss at 29000：1.1031\n",
      "-----------30000-----------\n",
      "Accuracy at 30000: 77.88%\n",
      "Loss at 30000：1.0825\n",
      "-----------31000-----------\n",
      "Accuracy at 31000: 78.25%\n",
      "Loss at 31000：1.0579\n",
      "-----------32000-----------\n",
      "Accuracy at 32000: 78.60%\n",
      "Loss at 32000：1.0392\n",
      "-----------33000-----------\n",
      "Accuracy at 33000: 78.82%\n",
      "Loss at 33000：1.0225\n",
      "-----------34000-----------\n",
      "Accuracy at 34000: 78.80%\n",
      "Loss at 34000：1.0094\n",
      "-----------35000-----------\n",
      "Accuracy at 35000: 79.43%\n",
      "Loss at 35000：0.9806\n",
      "-----------36000-----------\n",
      "Accuracy at 36000: 79.34%\n",
      "Loss at 36000：0.9770\n",
      "-----------37000-----------\n",
      "Accuracy at 37000: 79.19%\n",
      "Loss at 37000：0.9667\n",
      "-----------38000-----------\n",
      "Accuracy at 38000: 79.75%\n",
      "Loss at 38000：0.9424\n",
      "-----------39000-----------\n",
      "Accuracy at 39000: 79.57%\n",
      "Loss at 39000：0.9425\n",
      "-----------40000-----------\n",
      "Accuracy at 40000: 79.51%\n",
      "Loss at 40000：0.9293\n",
      "-----------41000-----------\n",
      "Accuracy at 41000: 80.01%\n",
      "Loss at 41000：0.9115\n",
      "-----------42000-----------\n",
      "Accuracy at 42000: 79.79%\n",
      "Loss at 42000：0.9108\n",
      "-----------43000-----------\n",
      "Accuracy at 43000: 79.66%\n",
      "Loss at 43000：0.9033\n",
      "-----------44000-----------\n",
      "Accuracy at 44000: 80.11%\n",
      "Loss at 44000：0.8876\n",
      "-----------45000-----------\n",
      "Accuracy at 45000: 79.84%\n",
      "Loss at 45000：0.8886\n",
      "-----------46000-----------\n",
      "Accuracy at 46000: 79.73%\n",
      "Loss at 46000：0.8857\n",
      "-----------47000-----------\n",
      "Accuracy at 47000: 80.21%\n",
      "Loss at 47000：0.8634\n",
      "-----------48000-----------\n",
      "Accuracy at 48000: 79.95%\n",
      "Loss at 48000：0.8709\n",
      "-----------49000-----------\n",
      "Accuracy at 49000: 79.83%\n",
      "Loss at 49000：0.8669\n",
      "-----------50000-----------\n",
      "Accuracy at 50000: 80.23%\n",
      "Loss at 50000：0.8521\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "import Run_Prediction as rp\n",
    "\n",
    "max_length = 100\n",
    "\n",
    "with graph_combined.as_default():\n",
    "    saver_the_model = tf.train.Saver(max_to_keep = 1)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess, coord)\n",
    "        train_data_len = len(cnn_logits_list)\n",
    "        try:        \n",
    "            i = start_step\n",
    "            avg_accuracy = 0\n",
    "            avg_loss = 0\n",
    "            while i < 50000:    \n",
    "                #train the combined model \n",
    "                loss, combined_prediction, accuracy, _ = sess.run([cross_entropy,prediction,cal_accuracy, train_step],\n",
    "                                                                  feed_dict={x_logits:cnn_logits_list[i%train_data_len],\n",
    "                                                                             x_naive_bayes:naive_bayes_input_list[i%train_data_len],\n",
    "                                                                             y_:label_list[i%train_data_len]})          \n",
    "                avg_accuracy += accuracy\n",
    "                avg_loss += loss\n",
    "                i += 1\n",
    "                #print(\".\",end=\"\")\n",
    "                if i%1000 == 0:\n",
    "                    print(\"-----------%d-----------\"%i)\n",
    "                    print(\"Accuracy at %d: %.2f%%\"%(i,avg_accuracy/10))\n",
    "                    print(\"Loss at %d：%.4f\"%(i, avg_loss/1000))\n",
    "                    saver.save(sess,ckpt_dir_this+'combined_new_category',global_step=i)\n",
    "                    #log.add_log('train_accuracy',i, avg_accuracy/1000)\n",
    "                    #log.add_log('train_loss',i, avg_loss/1000)\n",
    "                    #log.SaveToFile() \n",
    "                    avg_accuracy = 0\n",
    "                    avg_loss = 0\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('done')\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "            print('quit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the naive bayes word vectors. It takes a few minutes.\n",
      "quit\n"
     ]
    }
   ],
   "source": [
    "import Run_Prediction as rp\n",
    "max_length = 100\n",
    "#np.set_printoptions(precision=4,suppress =False)\n",
    "\n",
    "data = NewsCategory(batch_size=batch_size,max_length=max_length,shuffle=False)\n",
    "cnn_logits_list = []\n",
    "naive_bayes_input_list = []\n",
    "label_list = [] \n",
    "with graph_cnn.as_default(): \n",
    "    with tf.Session() as sess:\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess, coord)    \n",
    "\n",
    "        #load the pretrained cnn model\n",
    "        model_file=tf.train.latest_checkpoint(ckpt_dir_cnn)\n",
    "        saver.restore(sess,model_file)\n",
    "\n",
    "        #load naive bayes model\n",
    "        word_matrix_file = \"naive_bayes_word_matrix.csv\"\n",
    "        word_matrix, word_list = rp.read_naive_bayes_word_vector(word_matrix_file)\n",
    "\n",
    "        try:        \n",
    "            i = start_step\n",
    "            avg_accuracy = 0\n",
    "            while i < int(data.max_recorder/batch_size)+1:    \n",
    "                batch_data, batch_label = data.get_batch_data()  \n",
    "                batch_data_vec, data_length = word2vector(batch_data, model,max_length)\n",
    "\n",
    "                cnn_logits = sess.run([output_y],feed_dict={input_x:batch_data_vec, input_y:batch_label, x_length:data_length})\n",
    "                cnn_logits_list.append(cnn_logits[0])\n",
    "\n",
    "                naive_bayes_input = rp.naive_bayes_model(batch_data,word_matrix, word_list)\n",
    "                naive_bayes_input_list.append(naive_bayes_input)\n",
    "\n",
    "                label_list.append(batch_label)\n",
    "                i += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('done')\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "            coord.join(threads)\n",
    "            print('quit')        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./ckpt_save/combined_new_category-50000\n",
      "Restore parameters\n",
      "Accuracy: 80.07%\n"
     ]
    }
   ],
   "source": [
    "avg_accuracy = 0\n",
    "with graph_combined.as_default(): \n",
    "    saver = tf.train.Saver(max_to_keep = 1)\n",
    "    '''\n",
    "    saver = tf.train.import_meta_graph(ckpt_dir_this+'combined_new_category-50000.meta')\n",
    "    \n",
    "    x_logits = graph.get_tensor_by_name(\"Combined_Input/x_logits:0\")\n",
    "    x_naive_bayes = graph.get_tensor_by_name(\"Combined_Input/x_naive_bayes:0\")\n",
    "    y_ = graph.get_tensor_by_name(\"Combined_Input/y_:0\")\n",
    "    prediction = graph.get_tensor_by_name(\"Combined_Prediction/prediction:0\")\n",
    "    cal_accuracy = graph.get_tensor_by_name(\"Combined_Prediction/cal_accuracy:0\")\n",
    "    '''\n",
    "    with tf.Session() as sess:\n",
    "        model_file=tf.train.latest_checkpoint(ckpt_dir_this)\n",
    "        print(model_file)\n",
    "        saver.restore(sess, model_file)   \n",
    "        print('Restore model.')\n",
    "        for i in range(len(cnn_logits_list)):\n",
    "            combined_prediction, accuracy= sess.run([prediction,cal_accuracy],\n",
    "                                                    feed_dict={x_logits:cnn_logits_list[i],\n",
    "                                                               x_naive_bayes:naive_bayes_input_list[i],\n",
    "                                                               y_:label_list[i]})\n",
    "            avg_accuracy += accuracy\n",
    "        print(\"Accuracy: %.2f%%\"%(avg_accuracy/i*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3-tensorflow-gpu] *",
   "language": "python",
   "name": "conda-env-python3-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
