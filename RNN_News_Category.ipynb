{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import threading\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "import nltk\n",
    "sys.path.append('../../common/')\n",
    "\n",
    "from train_log import train_log\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1,2\" \n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.FATAL)\n",
    "\n",
    "FILE_NAME = 'News_Category_Dataset_v2.json'\n",
    "\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "glove_file = datapath('/home/ubuntu/Notebooks/GloVe/glove.6B.100d.txt')\n",
    "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")\n",
    "glove2word2vec(glove_file, word2vec_glove_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
    "\n",
    "ckpt_dir = './ckpt_save_cnn/'\n",
    "if not os.path.exists(ckpt_dir):\n",
    "    os.makedirs(ckpt_dir)\n",
    "log_dir = './log_cnn/'\n",
    "log = train_log(log_dir)\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "CONTINUE = 1\n",
    "start_step = 150000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getvector(word, model=model):\n",
    "    if model.vocab.get(word.lower(),\"NaN\") == \"NaN\":\n",
    "        lst = nltk.stem.LancasterStemmer()\n",
    "        if model.vocab.get(lst.stem(word).lower(),\"NaN\") == \"NaN\":\n",
    "            if word[-1] == \"s\" and model.vocab.get(word[:-1].lower(),\"NaN\") != \"NaN\":\n",
    "                return model[word[:-1].lower()]\n",
    "            else:\n",
    "                return model[\"unk\"]\n",
    "        else:\n",
    "            return model[lst.stem(word).lower()]\n",
    "    else:\n",
    "        return model[word.lower()]\n",
    "\n",
    "def word2vector(reviews, model=model, max_length=1000):\n",
    "    vector_data = np.zeros((len(reviews), max_length,100))\n",
    "    x_length = []\n",
    "    i = 0\n",
    "    for review in reviews:\n",
    "        j = 0 \n",
    "        if len(review) > max_length:\n",
    "            print(\"The length of the reviews is %dwhich is larger than max_length (%d)\"%(len(review),max_length))\n",
    "            print(review)\n",
    "            print(\"-\"*10)\n",
    "        for word in review:\n",
    "            vector_data[i,j] = getvector(word,model)\n",
    "            if str(vector_data[i,j,0])=='nan':\n",
    "                print(word)\n",
    "                break\n",
    "            j += 1\n",
    "        x_length.append(j)\n",
    "        i += 1\n",
    "    return vector_data, np.asarray(x_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUNK_DIC = []\\nKNW_DIC = []\\ncount = 0\\nfor item in data.data:\\n    for word in item:\\n        word_vec = getvector2(word, model)\\n        if word_vec == \"unk\":\\n            if word not in UNK_DIC:\\n                UNK_DIC.append(word)\\n        else:\\n            if word not in KNW_DIC:\\n                KNW_DIC.append(word)\\n        #print(word_vec,end=\" \")\\n    #print()\\n    count += 1\\n\\nprint(\"Unknown words %d in total %d words.\"%(len(UNK_DIC),len(KNW_DIC)))\\n\\nfor word in UNK_DIC:\\n    print(word,end=\", \")'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from NewsCategoryData import NewsCategory\n",
    "\n",
    "'''\n",
    "data = NewsCategory(batch_size=128)\n",
    "\n",
    "for i in range(10):\n",
    "    train_data, train_label = data.get_batch_data()\n",
    "    batch_data, data_length = word2vector(train_data, model)\n",
    "    #print(len(train_data),len(train_label))\n",
    "    #print(\"-\"*20)\n",
    "    #print(train_data, train_label)\n",
    "  \n",
    "max_len = 0\n",
    "for text in data.data:\n",
    "    if len(text) > max_len:\n",
    "        max_len = len(text)\n",
    "print(max_len)\n",
    "'''\n",
    "def getvector2(word, model=model):\n",
    "    if model.vocab.get(word.lower(),\"NaN\") == \"NaN\":\n",
    "        lst = nltk.stem.LancasterStemmer()\n",
    "        if model.vocab.get(lst.stem(word).lower(),\"NaN\") == \"NaN\":\n",
    "            if word[-1] == \"s\" and model.vocab.get(word[:-1].lower(),\"NaN\") != \"NaN\":\n",
    "                #print(word[:-1])\n",
    "                return word[:-1].lower()\n",
    "            else:\n",
    "                return \"unk\"\n",
    "        else:\n",
    "            return lst.stem(word).lower()\n",
    "    else:\n",
    "        return word.lower()\n",
    "#print(getvector2(\"2018s\",model))\n",
    "'''\n",
    "UNK_DIC = []\n",
    "KNW_DIC = []\n",
    "count = 0\n",
    "for item in data.data:\n",
    "    for word in item:\n",
    "        word_vec = getvector2(word, model)\n",
    "        if word_vec == \"unk\":\n",
    "            if word not in UNK_DIC:\n",
    "                UNK_DIC.append(word)\n",
    "        else:\n",
    "            if word not in KNW_DIC:\n",
    "                KNW_DIC.append(word)\n",
    "        #print(word_vec,end=\" \")\n",
    "    #print()\n",
    "    count += 1\n",
    "\n",
    "print(\"Unknown words %d in total %d words.\"%(len(UNK_DIC),len(KNW_DIC)))\n",
    "\n",
    "for word in UNK_DIC:\n",
    "    print(word,end=\", \")'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f66b01a25c0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicRNNCell object at 0x7f66b01a25c0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "state_size = 64\n",
    "max_length = 100\n",
    "n_classes = 41\n",
    "word2vector_len = 100\n",
    "\n",
    "with tf.variable_scope('Input'):\n",
    "    x = tf.placeholder(tf.float32, [None, max_length, word2vector_len], name=\"x\")\n",
    "    x_length = tf.placeholder(tf.int32, [None], name=\"x_length\")\n",
    "    y_ = tf.placeholder(tf.int64, [None],  name=\"y_\")\n",
    "    \n",
    "    #RNN的初始化状态，全设为零。注意state是与input保持一致，接下来会有concat操作，所以这里要有batch的维度。即每个样本都要有隐层状态\n",
    "    init_state = tf.zeros([batch_size, state_size], name=\"init_state\")\n",
    "\n",
    "with tf.variable_scope('RNN'):\n",
    "    #定义rnn_cell的权重参数，\n",
    "    cell = tf.contrib.rnn.BasicRNNCell(state_size, name = \"cell\")\n",
    "    #rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_stateu\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, inputs = x, \n",
    "                                                 dtype=tf.float32,\n",
    "                                                 #initial_state=init_state,\n",
    "                                                 sequence_length = x_length)\n",
    "with tf.variable_scope('fclayer'):\n",
    "    w1 = tf.Variable(tf.random_uniform([state_size, int(state_size*9)],0,1.0),name=\"w1\")\n",
    "    b1= tf.Variable(tf.zeros([int(state_size*9)]),name=\"b1\")\n",
    "    hidden = tf.sigmoid(tf.add(tf.matmul(final_state, w1), b1),name=\"hidden\")\n",
    "    #print(y.shape)\n",
    "\n",
    "with tf.variable_scope('Output'):\n",
    "    w2 = tf.Variable(tf.random_uniform([int(state_size*9), n_classes],0,1.0),name=\"w2\")\n",
    "    b2= tf.Variable(tf.zeros([n_classes]),name=\"b2\")\n",
    "    y = tf.add(tf.matmul(hidden, w2), b2,name=\"y\")\n",
    "    \n",
    "with tf.variable_scope('Loss'):\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,\n",
    "                                                                                  labels=y_, \n",
    "                                                                                  name=\"cross_entropy_loss\"))\n",
    "    #cross_entropy_3 = tf.cast(tf.one_hot(y_-1,n_classes),tf.float32)*tf.log(tf.nn.softmax(y))\n",
    "with tf.variable_scope('Prediction'):\n",
    "    prediction = tf.argmax(tf.nn.softmax(y),1, name=\"prediction\")\n",
    "    cal_accuracy = tf.reduce_mean(tf.cast(tf.equal(prediction,y_), tf.float32),name=\"cal_accuracy\")\n",
    "    \n",
    "with tf.variable_scope('Train'):\n",
    "    #train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "    train_step = tf.train.AdamOptimizer(0.000000001).minimize(cross_entropy)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 100, 64)\n",
      "(?, 64)\n",
      "(?, 41)\n"
     ]
    }
   ],
   "source": [
    "print(rnn_outputs.shape)\n",
    "print(final_state.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "At least two variables have the same name: Output/b2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-48e110ef965a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_to_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcoord\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCoordinator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3-tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, var_list, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, saver_def, builder, defer_build, allow_empty, write_version, pad_step_number, save_relative_paths, filename)\u001b[0m\n\u001b[1;32m    823\u001b[0m           time.time() + self._keep_checkpoint_every_n_hours * 3600)\n\u001b[1;32m    824\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdefer_build\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_saver_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3-tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    835\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Use save/restore instead of build in eager mode.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_build_eager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuild_restore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3-tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self, checkpoint_path, build_save, build_restore)\u001b[0m\n\u001b[1;32m    873\u001b[0m           \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    874\u001b[0m           \u001b[0mbuild_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_save\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 875\u001b[0;31m           build_restore=build_restore)\n\u001b[0m\u001b[1;32m    876\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    877\u001b[0m       \u001b[0;31m# Since self._name is used as a name_scope by builder(), we are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3-tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36m_build_internal\u001b[0;34m(self, names_to_saveables, reshape, sharded, max_to_keep, keep_checkpoint_every_n_hours, name, restore_sequentially, filename, build_save, build_restore)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     saveables = saveable_object_util.validate_and_slice_inputs(\n\u001b[0;32m--> 482\u001b[0;31m         names_to_saveables)\n\u001b[0m\u001b[1;32m    483\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmax_to_keep\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m       \u001b[0mmax_to_keep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3-tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/saving/saveable_object_util.py\u001b[0m in \u001b[0;36mvalidate_and_slice_inputs\u001b[0;34m(names_to_saveables)\u001b[0m\n\u001b[1;32m    333\u001b[0m   \"\"\"\n\u001b[1;32m    334\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0mnames_to_saveables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop_list_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames_to_saveables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m   \u001b[0msaveables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3-tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/training/saving/saveable_object_util.py\u001b[0m in \u001b[0;36mop_list_to_dict\u001b[0;34m(op_list, convert_variable_to_tensor)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames_to_saveables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m           raise ValueError(\"At least two variables have the same name: %s\" %\n\u001b[0;32m--> 292\u001b[0;31m                            name)\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0mnames_to_saveables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: At least two variables have the same name: Output/b2"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(max_to_keep = 1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    coord = tf.train.Coordinator()\n",
    "    if CONTINUE != 0:\n",
    "        model_file=tf.train.latest_checkpoint(ckpt_dir)\n",
    "        saver.restore(sess,model_file)\n",
    "    threads = tf.train.start_queue_runners(sess, coord)\n",
    "    try:\n",
    "        data = NewsCategory(batch_size=batch_size,max_length=max_length)\n",
    "        i = start_step\n",
    "        avg_accuracy = 0\n",
    "        avg_loss = 0\n",
    "        while i < 200000:    \n",
    "            batch_data, batch_label = data.get_batch_data()  \n",
    "            batch_data, data_length = word2vector(batch_data, model,max_length)\n",
    "            rnn_state, state, pred_value, y_value, loss, accuracy, _ = sess.run([rnn_outputs, final_state, y, prediction, cross_entropy, cal_accuracy, train_step],\n",
    "                                            feed_dict={x:batch_data, y_:batch_label, x_length:data_length})\n",
    "            '''\n",
    "            print(\"-----------------------------\")\n",
    "            print(\"batch_data\")\n",
    "            print(batch_data)\n",
    "            print(\"rnn_state\")\n",
    "            print(rnn_state)\n",
    "            print(\"state\")\n",
    "            print(state)\n",
    "            print(\"y_value\")\n",
    "            print(y_value)\n",
    "            print(\"batch_label\")\n",
    "            print(batch_label)\n",
    "            print(\"pred_value\")\n",
    "            print(pred_value)\n",
    "            '''\n",
    "            avg_accuracy += accuracy\n",
    "            avg_loss += loss\n",
    "            i += 1\n",
    "            #print(\".\",end=\"\")\n",
    "            if i%1000 == 0:\n",
    "                print(\"-----------%d-----------\"%i)\n",
    "                print(\"Accuracy at %d: %.2f%%\"%(i,avg_accuracy/10))\n",
    "                print(\"Loss at %d：%.4f\"%(i, avg_loss/1000))\n",
    "                saver.save(sess,ckpt_dir+'news_category',global_step=i)\n",
    "                log.add_log('train_accuracy',i, avg_accuracy/1000)\n",
    "                log.add_log('train_loss',i, avg_loss/1000)\n",
    "                log.SaveToFile() \n",
    "                avg_accuracy = 0\n",
    "                avg_loss = 0\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('done')\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "        coord.join(threads)\n",
    "        print('quit')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label, predicton:  25 31\n",
      "label, predicton:  26 2\n",
      "label, predicton:  20 32\n",
      "label, predicton:  10 4\n",
      "label, predicton:  24 37\n",
      "label, predicton:  10 1\n",
      "label, predicton:  21 19\n",
      "label, predicton:  14 32\n",
      "label, predicton:  38 4\n",
      "label, predicton:  13 4\n",
      "label, predicton:  8 4\n",
      "label, predicton:  35 8\n",
      "label, predicton:  0 4\n",
      "label, predicton:  25 31\n",
      "label, predicton:  32 1\n",
      "label, predicton:  3 31\n",
      "label, predicton:  25 31\n",
      "label, predicton:  6 9\n",
      "label, predicton:  21 16\n",
      "label, predicton:  9 1\n",
      "label, predicton:  1 16\n",
      "label, predicton:  4 1\n",
      "label, predicton:  12 33\n",
      "label, predicton:  11 31\n",
      "label, predicton:  7 31\n",
      "label, predicton:  2 4\n",
      "label, predicton:  0 4\n",
      "label, predicton:  2 4\n",
      "label, predicton:  36 34\n",
      "label, predicton:  9 10\n",
      "label, predicton:  33 31\n",
      "label, predicton:  4 12\n",
      "label, predicton:  4 0\n",
      "label, predicton:  32 13\n",
      "label, predicton:  4 8\n",
      "label, predicton:  11 31\n",
      "label, predicton:  36 37\n",
      "label, predicton:  13 4\n",
      "label, predicton:  24 12\n",
      "label, predicton:  1 0\n",
      "label, predicton:  3 5\n",
      "label, predicton:  2 26\n",
      "label, predicton:  8 1\n",
      "label, predicton:  26 4\n",
      "label, predicton:  0 6\n",
      "label, predicton:  16 31\n",
      "label, predicton:  29 27\n",
      "label, predicton:  21 8\n",
      "label, predicton:  8 1\n",
      "label, predicton:  25 31\n",
      "label, predicton:  10 12\n",
      "label, predicton:  23 0\n",
      "label, predicton:  34 33\n",
      "label, predicton:  22 34\n",
      "label, predicton:  3 4\n",
      "label, predicton:  11 31\n",
      "label, predicton:  16 5\n",
      "label, predicton:  8 31\n",
      "label, predicton:  35 31\n",
      "label, predicton:  30 34\n",
      "label, predicton:  1 8\n",
      "label, predicton:  6 37\n",
      "label, predicton:  11 4\n",
      "label, predicton:  0 1\n",
      "label, predicton:  39 23\n",
      "label, predicton:  32 4\n",
      "label, predicton:  13 1\n",
      "label, predicton:  4 8\n",
      "label, predicton:  23 11\n",
      "label, predicton:  11 14\n",
      "label, predicton:  27 1\n",
      "label, predicton:  3 11\n",
      "label, predicton:  4 31\n",
      "label, predicton:  0 5\n",
      "label, predicton:  15 2\n",
      "label, predicton:  9 4\n",
      "label, predicton:  34 33\n",
      "label, predicton:  32 31\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "#max_length = 500\n",
    "with tf.Session() as sess:\n",
    "    graph = tf.get_default_graph()\n",
    "    saver = tf.train.import_meta_graph(ckpt_dir+'news_category-100000.meta')\n",
    "    model_file=tf.train.latest_checkpoint(ckpt_dir)\n",
    "    saver.restore(sess,model_file)\n",
    "    \n",
    "    x = graph.get_tensor_by_name(\"Input/x:0\")\n",
    "    y_ = graph.get_tensor_by_name(\"Input/y_:0\")\n",
    "    x_length = graph.get_tensor_by_name(\"Input/x_length:0\")\n",
    "    y = graph.get_tensor_by_name(\"Output/y:0\")\n",
    "    prediction = graph.get_tensor_by_name(\"Prediction/prediction:0\")\n",
    "    #cal_accuracy = graph.get_tensor_by_name(\"Prediction/cal_accuracy:0\")\n",
    "    data = NewsCategory(batch_size=batch_size,max_length=max_length)\n",
    "    i = 0\n",
    "    avg_accuracy = 0\n",
    "    while i < 10:\n",
    "        batch_data, batch_label = data.get_batch_data()  \n",
    "        batch_data_vec, data_length = word2vector(batch_data, model,max_length)\n",
    "        logits, pred_cate = sess.run([y, prediction],feed_dict={x:batch_data_vec, y_:batch_label, x_length:data_length})\n",
    "        for j in range(batch_size):\n",
    "            #print(\"logits:\",logits[j])\n",
    "            if batch_label[j] != pred_cate[j]:\n",
    "                #print(\"batch data:\",batch_data[j])\n",
    "                print(\"label, predicton: \",batch_label[j], pred_cate[j])\n",
    "        #avg_accuracy += accuracy[0]\n",
    "        i += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3-tensorflow-gpu] *",
   "language": "python",
   "name": "conda-env-python3-tensorflow-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
